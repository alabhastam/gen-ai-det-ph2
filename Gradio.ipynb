{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport math\nimport gradio as gr\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nfrom collections import Counter\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:41:49.638895Z","iopub.execute_input":"2025-11-23T19:41:49.639613Z","iopub.status.idle":"2025-11-23T19:41:49.643923Z","shell.execute_reply.started":"2025-11-23T19:41:49.639586Z","shell.execute_reply":"2025-11-23T19:41:49.643023Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# --- MODEL SETUP ---\n# We force CPU if CUDA is acting weird, but usually try CUDA first\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚è≥ Loading models on {device}...\")\n\ntry:\n    # Load GPT-2 for Perplexity\n    model_id = \"gpt2\"\n    tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n    model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n    model.eval()\n\n    # Load spaCy for POS\n    nlp = spacy.load(\"en_core_web_sm\")\n    print(\"‚úÖ Models loaded successfully.\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading models: {e}\")\n    print(\"Try restarting the session again if this persists.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:44:06.082625Z","iopub.execute_input":"2025-11-23T19:44:06.083660Z","iopub.status.idle":"2025-11-23T19:44:10.249298Z","shell.execute_reply.started":"2025-11-23T19:44:06.083624Z","shell.execute_reply":"2025-11-23T19:44:10.248118Z"}},"outputs":[{"name":"stdout","text":"‚è≥ Loading models on cpu...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1d8815ea6914d39b6a4b008cab47069"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"481000cf5f0e41acb9e69287f35579dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c2699a51744334b4a286c1d50eead2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24d85d4a6adc44cf9868aa340c132c23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c96fed430ba4536bdc50ade9e49ec71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8058e6d3e0524747998c8cab266a140c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe729ab1b70c4fe6989726c4814f11c2"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Models loaded successfully.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- FUNCTIONS ---\n\ndef calculate_perplexity(text):\n    if not text or len(text.strip()) == 0:\n        return 0\n        \n    encodings = tokenizer(text, return_tensors=\"pt\")\n    max_length = model.config.n_positions\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    \n    for begin_loc in range(0, seq_len, stride):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc \n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    if not nlls: return 0\n    ppl = torch.exp(torch.stack(nlls).mean())\n    return ppl.item()\n\ndef analyze_pos_features(text):\n    doc = nlp(text)\n    total_tokens = len(doc)\n    if total_tokens == 0: return {}, 0\n    \n    counts = Counter([token.pos_ for token in doc])\n    \n    ratios = {\n        \"Noun Ratio\": counts.get(\"NOUN\", 0) / total_tokens,\n        \"Verb Ratio\": counts.get(\"VERB\", 0) / total_tokens,\n        \"Adjective Ratio\": counts.get(\"ADJ\", 0) / total_tokens,\n        \"Pronoun Ratio\": counts.get(\"PRON\", 0) / total_tokens,\n    }\n    \n    probs = np.array([c / total_tokens for c in counts.values()])\n    entropy = -(probs * np.log2(probs)).sum()\n    \n    return ratios, entropy\n\ndef detect_ai_human(text):\n    if not text.strip():\n        return \"Please enter text.\", {}, {}, \"N/A\"\n\n    ppl = calculate_perplexity(text)\n    ratios, entropy = analyze_pos_features(text)\n    \n    # Heuristic Scoring\n    ai_score = 0\n    human_score = 0\n    \n    # Perplexity Logic\n    if ppl < 20: ai_score += 4\n    elif ppl < 30: ai_score += 2\n    elif ppl > 45: human_score += 4\n    elif ppl > 35: human_score += 2\n    \n    # Entropy Logic\n    if entropy < 3.32: ai_score += 1\n    elif entropy > 3.36: human_score += 1\n    \n    # Ratio Logic\n    if ratios[\"Noun Ratio\"] > 0.22: ai_score += 1\n    if ratios[\"Verb Ratio\"] > 0.12: human_score += 1\n    \n    if ai_score > human_score: verdict = \"Likely AI \"\n    elif human_score > ai_score: verdict = \"Likely Human \"\n    else: verdict = \"Uncertain \"\n\n    return verdict, f\"{ppl:.2f}\", f\"{entropy:.4f}\", ratios","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:46:19.868015Z","iopub.execute_input":"2025-11-23T19:46:19.868315Z","iopub.status.idle":"2025-11-23T19:46:19.881903Z","shell.execute_reply.started":"2025-11-23T19:46:19.868294Z","shell.execute_reply":"2025-11-23T19:46:19.880928Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- CORRECTED GRADIO UI CODE ---\nimport gradio as gr\n\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# üïµÔ∏è AI vs Human Text Analyzer\")\n    gr.Markdown(\"Server running on Kaggle T4 GPU.\")\n    \n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\", lines=8, placeholder=\"Type here...\")\n            analyze_btn = gr.Button(\"Analyze\", variant=\"primary\")\n        \n        with gr.Column():\n            lbl_verdict = gr.Label(label=\"Verdict\")\n            with gr.Row():\n                # We define the label variable here as lbl_ppl\n                lbl_ppl = gr.Textbox(label=\"Perplexity\") \n                lbl_entropy = gr.Textbox(label=\"Entropy\")\n            lbl_ratios = gr.JSON(label=\"POS Ratios\")\n\n    analyze_btn.click(\n        fn=detect_ai_human,\n        inputs=input_text,\n        # CORRECTED LINE BELOW: used 'lbl_ppl' instead of 'ppl'\n        outputs=[lbl_verdict, lbl_ppl, lbl_entropy, lbl_ratios]\n    )\n\n# Launch with server share enabled\ndemo.launch(share=True, debug=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:47:00.643632Z","iopub.execute_input":"2025-11-23T19:47:00.644330Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://c7d7ece413d3235e0a.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://c7d7ece413d3235e0a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":null}]}