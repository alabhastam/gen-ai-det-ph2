{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":656933,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":496575,"modelId":511974}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/gradio-user-interface?scriptVersionId=282388202\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nimport math\nimport gradio as gr\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nfrom collections import Counter\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- MODEL SETUP ---\n# We force CPU if CUDA is acting weird, but usually try CUDA first\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚è≥ Loading models on {device}...\")\n\ntry:\n    # Load GPT-2 for Perplexity\n    model_id = \"gpt2\"\n    tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n    model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n    model.eval()\n\n    # Load spaCy for POS\n    nlp = spacy.load(\"en_core_web_sm\")\n    print(\"‚úÖ Models loaded successfully.\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading models: {e}\")\n    print(\"Try restarting the session again if this persists.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- FUNCTIONS ---\n\ndef calculate_perplexity(text):\n    if not text or len(text.strip()) == 0:\n        return 0\n        \n    encodings = tokenizer(text, return_tensors=\"pt\")\n    max_length = model.config.n_positions\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    \n    for begin_loc in range(0, seq_len, stride):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc \n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    if not nlls: return 0\n    ppl = torch.exp(torch.stack(nlls).mean())\n    return ppl.item()\n\ndef analyze_pos_features(text):\n    doc = nlp(text)\n    total_tokens = len(doc)\n    if total_tokens == 0: return {}, 0\n    \n    counts = Counter([token.pos_ for token in doc])\n    \n    ratios = {\n        \"Noun Ratio\": counts.get(\"NOUN\", 0) / total_tokens,\n        \"Verb Ratio\": counts.get(\"VERB\", 0) / total_tokens,\n        \"Adjective Ratio\": counts.get(\"ADJ\", 0) / total_tokens,\n        \"Pronoun Ratio\": counts.get(\"PRON\", 0) / total_tokens,\n    }\n    \n    probs = np.array([c / total_tokens for c in counts.values()])\n    entropy = -(probs * np.log2(probs)).sum()\n    \n    return ratios, entropy\n\ndef detect_ai_human(text):\n    if not text.strip():\n        return \"Please enter text.\", {}, {}, \"N/A\"\n\n    ppl = calculate_perplexity(text)\n    ratios, entropy = analyze_pos_features(text)\n    \n    # Heuristic Scoring\n    ai_score = 0\n    human_score = 0\n    \n    # Perplexity Logic\n    if ppl < 20: ai_score += 4\n    elif ppl < 30: ai_score += 2\n    elif ppl > 45: human_score += 4\n    elif ppl > 35: human_score += 2\n    \n    # Entropy Logic\n    if entropy < 3.32: ai_score += 1\n    elif entropy > 3.36: human_score += 1\n    \n    # Ratio Logic\n    if ratios[\"Noun Ratio\"] > 0.22: ai_score += 1\n    if ratios[\"Verb Ratio\"] > 0.12: human_score += 1\n    \n    if ai_score > human_score: verdict = \"Likely AI \"\n    elif human_score > ai_score: verdict = \"Likely Human \"\n    else: verdict = \"Uncertain \"\n\n    return verdict, f\"{ppl:.2f}\", f\"{entropy:.4f}\", ratios","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CORRECTED GRADIO UI CODE ---\nimport gradio as gr\n\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# üïµÔ∏è AI vs Human Text Analyzer\")\n    gr.Markdown(\"Server running on Kaggle T4 GPU.\")\n    \n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\", lines=8, placeholder=\"Type here...\")\n            analyze_btn = gr.Button(\"Analyze\", variant=\"primary\")\n        \n        with gr.Column():\n            lbl_verdict = gr.Label(label=\"Verdict\")\n            with gr.Row():\n                # We define the label variable here as lbl_ppl\n                lbl_ppl = gr.Textbox(label=\"Perplexity\") \n                lbl_entropy = gr.Textbox(label=\"Entropy\")\n            lbl_ratios = gr.JSON(label=\"POS Ratios\")\n\n    analyze_btn.click(\n        fn=detect_ai_human,\n        inputs=input_text,\n        # CORRECTED LINE BELOW: used 'lbl_ppl' instead of 'ppl'\n        outputs=[lbl_verdict, lbl_ppl, lbl_entropy, lbl_ratios]\n    )\n\n# Launch with server share enabled\ndemo.launch(share=True, debug=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"code","source":"import torch\nimport math\nimport gradio as gr\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast, BertTokenizer, BertModel\nfrom collections import Counter\nimport torch.nn as nn\nimport re\nimport unicodedata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:40:35.614224Z","iopub.execute_input":"2025-11-28T09:40:35.614554Z","iopub.status.idle":"2025-11-28T09:41:22.476209Z","shell.execute_reply.started":"2025-11-28T09:40:35.61453Z","shell.execute_reply":"2025-11-28T09:41:22.474991Z"}},"outputs":[{"name":"stderr","text":"2025-11-28 09:40:57.231397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764322857.461171     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764322857.524001     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- MODEL SETUP ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚è≥ Loading models on {device}...\")\n\ntry:\n    # Load GPT-2 for Perplexity\n    model_id = \"gpt2\"\n    gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n    gpt2_model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n    gpt2_model.eval()\n\n    # Load spaCy for POS\n    nlp = spacy.load(\"en_core_web_sm\")\n    \n    # Load BERT tokenizer and define model class\n    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n    \n    class BERTClassifier(nn.Module):\n        def __init__(self, model_name):\n            super(BERTClassifier, self).__init__()\n            self.bert = BertModel.from_pretrained(model_name)\n            self.drop = nn.Dropout(p=0.3)\n            self.out = nn.Linear(in_features=768, out_features=2)\n            \n        def forward(self, input_ids, attention_mask):\n            output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n            pooled_output = output[1]\n            output = self.drop(pooled_output)\n            return self.out(output)\n    \n    # Initialize BERT model and load trained weights\n    bert_model = BERTClassifier('bert-base-cased').to(device)\n    bert_model.load_state_dict(torch.load('/kaggle/input/aigen-bertmodel/pytorch/default/1/bert_model_state.bin', map_location=device))\n    bert_model.eval()\n    \n    print(\"All models loaded successfully.\")\nexcept Exception as e:\n    print(f\" Error loading models: {e}\")\n    print(\"Try restarting the session if this persists.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:44:16.97029Z","iopub.execute_input":"2025-11-28T09:44:16.971258Z","iopub.status.idle":"2025-11-28T09:44:28.396791Z","shell.execute_reply.started":"2025-11-28T09:44:16.971226Z","shell.execute_reply":"2025-11-28T09:44:28.395571Z"}},"outputs":[{"name":"stdout","text":"‚è≥ Loading models on cpu...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"519978ff7e0b4cf1a63eb260f143487c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7c4ffbf1fa4dbeb835e2952ab1bb45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e29aecf6cd04bf789687e27888b2c10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebb8a42f34bb4ab5a7c320080f51122a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab80cd045ee34aacac0bfbd3ed5efa27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f54ccceec14224ba62ad839c5fa4fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4256001889a432fb85dcd551202ce15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ac8e9a33b7429e9313fed105649e7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdaf8f65687c4e53bfe5d22114cbbf7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0771d00a44144abc8f3e2838f1d6ef70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f18ab7b8986147968b80f5304575a035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cedbe0805334acdb0a0887771c5a866"}},"metadata":{}},{"name":"stdout","text":"All models loaded successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- TEXT PREPROCESSING ---\ndef preprocess_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    \n    text = unicodedata.normalize('NFKC', text)\n    text = re.sub(r'<.*?>', '', text)\n    text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n    text = text.replace('\\\\', '')\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:44:45.935565Z","iopub.execute_input":"2025-11-28T09:44:45.935969Z","iopub.status.idle":"2025-11-28T09:44:45.942001Z","shell.execute_reply.started":"2025-11-28T09:44:45.935939Z","shell.execute_reply":"2025-11-28T09:44:45.940729Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# --- BERT PREDICTION ---\ndef predict_with_bert(text, max_len=512):\n    \"\"\"Returns probability that text is AI-generated\"\"\"\n    if not text.strip():\n        return 0.5, \"No text provided\"\n    \n    cleaned_text = preprocess_text(text)\n    \n    encoding = bert_tokenizer.encode_plus(\n        cleaned_text,\n        add_special_tokens=True,\n        max_length=max_len,\n        return_token_type_ids=False,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt',\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n        probs = torch.softmax(outputs, dim=1)\n        ai_prob = probs[0][1].item()  # Probability of AI class\n        \n    prediction = \"AI-generated\" if ai_prob > 0.5 else \"Human-written\"\n    confidence = max(ai_prob, 1 - ai_prob) * 100\n    \n    return ai_prob, f\"{prediction} ({confidence:.1f}% confidence)\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:46:16.508525Z","iopub.execute_input":"2025-11-28T09:46:16.50889Z","iopub.status.idle":"2025-11-28T09:46:16.516992Z","shell.execute_reply.started":"2025-11-28T09:46:16.508862Z","shell.execute_reply":"2025-11-28T09:46:16.515958Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# --- PERPLEXITY CALCULATION ---\ndef calculate_perplexity(text):\n    if not text or len(text.strip()) == 0:\n        return 0\n        \n    encodings = gpt2_tokenizer(text, return_tensors=\"pt\")\n    max_length = gpt2_model.config.n_positions\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    \n    for begin_loc in range(0, seq_len, stride):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc \n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        with torch.no_grad():\n            outputs = gpt2_model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    if not nlls: \n        return 0\n    ppl = torch.exp(torch.stack(nlls).mean())\n    return ppl.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:46:33.927011Z","iopub.execute_input":"2025-11-28T09:46:33.927994Z","iopub.status.idle":"2025-11-28T09:46:33.935638Z","shell.execute_reply.started":"2025-11-28T09:46:33.927963Z","shell.execute_reply":"2025-11-28T09:46:33.934743Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- POS ANALYSIS ---\ndef analyze_pos_features(text):\n    doc = nlp(text)\n    total_tokens = len(doc)\n    if total_tokens == 0: \n        return {}, 0\n    \n    counts = Counter([token.pos_ for token in doc])\n    \n    ratios = {\n        \"Noun Ratio\": counts.get(\"NOUN\", 0) / total_tokens,\n        \"Verb Ratio\": counts.get(\"VERB\", 0) / total_tokens,\n        \"Adjective Ratio\": counts.get(\"ADJ\", 0) / total_tokens,\n        \"Pronoun Ratio\": counts.get(\"PRON\", 0) / total_tokens,\n    }\n    \n    probs = np.array([c / total_tokens for c in counts.values()])\n    entropy = -(probs * np.log2(probs)).sum()\n    \n    return ratios, entropy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:47:22.725033Z","iopub.execute_input":"2025-11-28T09:47:22.725827Z","iopub.status.idle":"2025-11-28T09:47:22.732589Z","shell.execute_reply.started":"2025-11-28T09:47:22.725792Z","shell.execute_reply":"2025-11-28T09:47:22.731408Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def detect_ai_human(text):\n    if not text.strip():\n        return \"Please enter text.\", {}, {}, \"N/A\", \"N/A\", \"N/A\"\n\n    # BERT prediction\n    ai_prob, bert_verdict = predict_with_bert(text)\n    \n    # Perplexity\n    ppl = calculate_perplexity(text)\n    \n    # POS analysis\n    ratios, entropy = analyze_pos_features(text)\n    \n    # Heuristic scoring (from original code)\n    ai_score = 0\n    human_score = 0\n    \n    if ppl < 20: \n        ai_score += 4\n    elif ppl < 30: \n        ai_score += 2\n    elif ppl > 45: \n        human_score += 4\n    elif ppl > 35: \n        human_score += 2\n    \n    if entropy < 3.32: \n        ai_score += 1\n    elif entropy > 3.36: \n        human_score += 1\n    \n    if ratios[\"Noun Ratio\"] > 0.22: \n        ai_score += 1\n    if ratios[\"Verb Ratio\"] > 0.12: \n        human_score += 1\n    \n    if ai_score > human_score: \n        heuristic_verdict = \"Likely AI\"\n    elif human_score > ai_score: \n        heuristic_verdict = \"Likely Human\"\n    else: \n        heuristic_verdict = \"Uncertain\"\n\n    return bert_verdict, heuristic_verdict, f\"{ppl:.2f}\", f\"{entropy:.4f}\", f\"{ai_prob:.4f}\", ratios","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:47:55.274889Z","iopub.execute_input":"2025-11-28T09:47:55.275388Z","iopub.status.idle":"2025-11-28T09:47:55.282808Z","shell.execute_reply.started":"2025-11-28T09:47:55.275363Z","shell.execute_reply":"2025-11-28T09:47:55.281735Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- GRADIO UI ---\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# üïµÔ∏è AI vs Human Text Analyzer\")\n    gr.Markdown(\"Combines BERT classifier with perplexity and linguistic analysis. Running on Kaggle T4 GPU.\")\n    \n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\", lines=10, placeholder=\"Paste text here for analysis...\")\n            analyze_btn = gr.Button(\"üîç Analyze Text\", variant=\"primary\", size=\"lg\")\n        \n        with gr.Column():\n            gr.Markdown(\"### ü§ñ BERT Model Prediction\")\n            lbl_bert = gr.Textbox(label=\"BERT Verdict\", interactive=False)\n            lbl_bert_prob = gr.Textbox(label=\"AI Probability\", interactive=False)\n            \n            gr.Markdown(\"### üìä Heuristic Analysis\")\n            lbl_heuristic = gr.Textbox(label=\"Heuristic Verdict\", interactive=False)\n            \n            with gr.Row():\n                lbl_ppl = gr.Textbox(label=\"Perplexity\", interactive=False)\n                lbl_entropy = gr.Textbox(label=\"POS Entropy\", interactive=False)\n            \n            lbl_ratios = gr.JSON(label=\"Part-of-Speech Ratios\")\n\n    analyze_btn.click(\n        fn=detect_ai_human,\n        inputs=input_text,\n        outputs=[lbl_bert, lbl_heuristic, lbl_ppl, lbl_entropy, lbl_bert_prob, lbl_ratios]\n    )\n\n# Launch\ndemo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:48:23.465299Z","iopub.execute_input":"2025-11-28T09:48:23.466455Z","iopub.status.idle":"2025-11-28T10:18:24.467109Z","shell.execute_reply.started":"2025-11-28T09:48:23.466412Z","shell.execute_reply":"2025-11-28T10:18:24.466061Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://91cf4a521477490328.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://91cf4a521477490328.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://91cf4a521477490328.gradio.live\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Add this debug function to check what your model actually learned\ndef debug_bert_predictions(text):\n    cleaned_text = preprocess_text(text)\n    \n    encoding = bert_tokenizer.encode_plus(\n        cleaned_text,\n        add_special_tokens=True,\n        max_length=512,\n        return_token_type_ids=False,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt',\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n        probs = torch.softmax(outputs, dim=1)\n        \n        print(f\"Raw logits: {outputs[0]}\")\n        print(f\"Class 0 prob: {probs[0][0].item():.4f}\")\n        print(f\"Class 1 prob: {probs[0][1].item():.4f}\")\n        \n    return probs\n\n# Test with known human text\ndebug_bert_predictions(\"HIIIIIIIIIIIIII Its meeeeeeeeeee\")\n# Test with known AI text  \ndebug_bert_predictions(\"The implementation of artificial intelligence systems requires careful consideration.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:21:07.002102Z","iopub.execute_input":"2025-11-28T10:21:07.002884Z","iopub.status.idle":"2025-11-28T10:21:08.731155Z","shell.execute_reply.started":"2025-11-28T10:21:07.002855Z","shell.execute_reply":"2025-11-28T10:21:08.730169Z"}},"outputs":[{"name":"stdout","text":"Raw logits: tensor([-3.6124,  3.6595])\nClass 0 prob: 0.0007\nClass 1 prob: 0.9993\nRaw logits: tensor([-6.0712,  6.2176])\nClass 0 prob: 0.0000\nClass 1 prob: 1.0000\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"tensor([[4.6031e-06, 1.0000e+00]])"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# After loading, verify the weights aren't just random\nprint(\"First layer weight sample:\", bert_model.bert.embeddings.word_embeddings.weight[0][:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:20:26.978807Z","iopub.execute_input":"2025-11-28T10:20:26.979484Z","iopub.status.idle":"2025-11-28T10:20:26.988139Z","shell.execute_reply.started":"2025-11-28T10:20:26.979446Z","shell.execute_reply":"2025-11-28T10:20:26.987271Z"}},"outputs":[{"name":"stdout","text":"First layer weight sample: tensor([-0.0005, -0.0415,  0.0131,  0.0058, -0.0376], grad_fn=<SliceBackward0>)\n","output_type":"stream"}],"execution_count":12}]}