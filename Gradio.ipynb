{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":656933,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":496575,"modelId":511974}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/gradio-user-interface?scriptVersionId=282388202\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nimport math\nimport gradio as gr\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nfrom collections import Counter\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- MODEL SETUP ---\n# We force CPU if CUDA is acting weird, but usually try CUDA first\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚è≥ Loading models on {device}...\")\n\ntry:\n    # Load GPT-2 for Perplexity\n    model_id = \"gpt2\"\n    tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n    model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n    model.eval()\n\n    # Load spaCy for POS\n    nlp = spacy.load(\"en_core_web_sm\")\n    print(\"‚úÖ Models loaded successfully.\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading models: {e}\")\n    print(\"Try restarting the session again if this persists.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- FUNCTIONS ---\n\ndef calculate_perplexity(text):\n    if not text or len(text.strip()) == 0:\n        return 0\n        \n    encodings = tokenizer(text, return_tensors=\"pt\")\n    max_length = model.config.n_positions\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    \n    for begin_loc in range(0, seq_len, stride):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc \n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    if not nlls: return 0\n    ppl = torch.exp(torch.stack(nlls).mean())\n    return ppl.item()\n\ndef analyze_pos_features(text):\n    doc = nlp(text)\n    total_tokens = len(doc)\n    if total_tokens == 0: return {}, 0\n    \n    counts = Counter([token.pos_ for token in doc])\n    \n    ratios = {\n        \"Noun Ratio\": counts.get(\"NOUN\", 0) / total_tokens,\n        \"Verb Ratio\": counts.get(\"VERB\", 0) / total_tokens,\n        \"Adjective Ratio\": counts.get(\"ADJ\", 0) / total_tokens,\n        \"Pronoun Ratio\": counts.get(\"PRON\", 0) / total_tokens,\n    }\n    \n    probs = np.array([c / total_tokens for c in counts.values()])\n    entropy = -(probs * np.log2(probs)).sum()\n    \n    return ratios, entropy\n\ndef detect_ai_human(text):\n    if not text.strip():\n        return \"Please enter text.\", {}, {}, \"N/A\"\n\n    ppl = calculate_perplexity(text)\n    ratios, entropy = analyze_pos_features(text)\n    \n    # Heuristic Scoring\n    ai_score = 0\n    human_score = 0\n    \n    # Perplexity Logic\n    if ppl < 20: ai_score += 4\n    elif ppl < 30: ai_score += 2\n    elif ppl > 45: human_score += 4\n    elif ppl > 35: human_score += 2\n    \n    # Entropy Logic\n    if entropy < 3.32: ai_score += 1\n    elif entropy > 3.36: human_score += 1\n    \n    # Ratio Logic\n    if ratios[\"Noun Ratio\"] > 0.22: ai_score += 1\n    if ratios[\"Verb Ratio\"] > 0.12: human_score += 1\n    \n    if ai_score > human_score: verdict = \"Likely AI \"\n    elif human_score > ai_score: verdict = \"Likely Human \"\n    else: verdict = \"Uncertain \"\n\n    return verdict, f\"{ppl:.2f}\", f\"{entropy:.4f}\", ratios","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CORRECTED GRADIO UI CODE ---\nimport gradio as gr\n\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# üïµÔ∏è AI vs Human Text Analyzer\")\n    gr.Markdown(\"Server running on Kaggle T4 GPU.\")\n    \n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\", lines=8, placeholder=\"Type here...\")\n            analyze_btn = gr.Button(\"Analyze\", variant=\"primary\")\n        \n        with gr.Column():\n            lbl_verdict = gr.Label(label=\"Verdict\")\n            with gr.Row():\n                # We define the label variable here as lbl_ppl\n                lbl_ppl = gr.Textbox(label=\"Perplexity\") \n                lbl_entropy = gr.Textbox(label=\"Entropy\")\n            lbl_ratios = gr.JSON(label=\"POS Ratios\")\n\n    analyze_btn.click(\n        fn=detect_ai_human,\n        inputs=input_text,\n        # CORRECTED LINE BELOW: used 'lbl_ppl' instead of 'ppl'\n        outputs=[lbl_verdict, lbl_ppl, lbl_entropy, lbl_ratios]\n    )\n\n# Launch with server share enabled\ndemo.launch(share=True, debug=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"code","source":"import torch\nimport math\nimport gradio as gr\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast, BertTokenizer, BertModel\nfrom collections import Counter\nimport torch.nn as nn\nimport re\nimport unicodedata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:40:35.614224Z","iopub.execute_input":"2025-11-28T09:40:35.614554Z","iopub.status.idle":"2025-11-28T09:41:22.476209Z","shell.execute_reply.started":"2025-11-28T09:40:35.61453Z","shell.execute_reply":"2025-11-28T09:41:22.474991Z"}},"outputs":[{"name":"stderr","text":"2025-11-28 09:40:57.231397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764322857.461171     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764322857.524001     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- MODEL SETUP ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚è≥ Loading models on {device}...\")\n\ntry:\n    # Load GPT-2 for Perplexity\n    model_id = \"gpt2\"\n    gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n    gpt2_model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n    gpt2_model.eval()\n\n    # Load spaCy for POS\n    nlp = spacy.load(\"en_core_web_sm\")\n    \n    # Load BERT tokenizer and define model class\n    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n    \n    class BERTClassifier(nn.Module):\n        def __init__(self, model_name):\n            super(BERTClassifier, self).__init__()\n            self.bert = BertModel.from_pretrained(model_name)\n            self.drop = nn.Dropout(p=0.3)\n            self.out = nn.Linear(in_features=768, out_features=2)\n            \n        def forward(self, input_ids, attention_mask):\n            output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n            pooled_output = output[1]\n            output = self.drop(pooled_output)\n            return self.out(output)\n    \n    # Initialize BERT model and load trained weights\n    bert_model = BERTClassifier('bert-base-cased').to(device)\n    bert_model.load_state_dict(torch.load('/kaggle/input/aigen-bertmodel/pytorch/default/1/bert_model_state.bin', map_location=device))\n    bert_model.eval()\n    \n    print(\"All models loaded successfully.\")\nexcept Exception as e:\n    print(f\" Error loading models: {e}\")\n    print(\"Try restarting the session if this persists.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:44:16.97029Z","iopub.execute_input":"2025-11-28T09:44:16.971258Z","iopub.status.idle":"2025-11-28T09:44:28.396791Z","shell.execute_reply.started":"2025-11-28T09:44:16.971226Z","shell.execute_reply":"2025-11-28T09:44:28.395571Z"}},"outputs":[{"name":"stdout","text":"‚è≥ Loading models on cpu...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"519978ff7e0b4cf1a63eb260f143487c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7c4ffbf1fa4dbeb835e2952ab1bb45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e29aecf6cd04bf789687e27888b2c10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebb8a42f34bb4ab5a7c320080f51122a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab80cd045ee34aacac0bfbd3ed5efa27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f54ccceec14224ba62ad839c5fa4fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4256001889a432fb85dcd551202ce15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ac8e9a33b7429e9313fed105649e7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdaf8f65687c4e53bfe5d22114cbbf7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0771d00a44144abc8f3e2838f1d6ef70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f18ab7b8986147968b80f5304575a035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cedbe0805334acdb0a0887771c5a866"}},"metadata":{}},{"name":"stdout","text":"All models loaded successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- TEXT PREPROCESSING ---\ndef preprocess_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    \n    text = unicodedata.normalize('NFKC', text)\n    text = re.sub(r'<.*?>', '', text)\n    text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n    text = text.replace('\\\\', '')\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:44:45.935565Z","iopub.execute_input":"2025-11-28T09:44:45.935969Z","iopub.status.idle":"2025-11-28T09:44:45.942001Z","shell.execute_reply.started":"2025-11-28T09:44:45.935939Z","shell.execute_reply":"2025-11-28T09:44:45.940729Z"}},"outputs":[],"execution_count":3}]}