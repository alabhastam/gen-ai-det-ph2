{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":656933,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":496575,"modelId":511974}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/gradio-user-interface?scriptVersionId=282424374\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nimport math\nimport gradio as gr\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nfrom collections import Counter\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- MODEL SETUP ---\n# We force CPU if CUDA is acting weird, but usually try CUDA first\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚è≥ Loading models on {device}...\")\n\ntry:\n    # Load GPT-2 for Perplexity\n    model_id = \"gpt2\"\n    tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n    model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n    model.eval()\n\n    # Load spaCy for POS\n    nlp = spacy.load(\"en_core_web_sm\")\n    print(\"‚úÖ Models loaded successfully.\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading models: {e}\")\n    print(\"Try restarting the session again if this persists.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- FUNCTIONS ---\n\ndef calculate_perplexity(text):\n    if not text or len(text.strip()) == 0:\n        return 0\n        \n    encodings = tokenizer(text, return_tensors=\"pt\")\n    max_length = model.config.n_positions\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    \n    for begin_loc in range(0, seq_len, stride):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc \n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    if not nlls: return 0\n    ppl = torch.exp(torch.stack(nlls).mean())\n    return ppl.item()\n\ndef analyze_pos_features(text):\n    doc = nlp(text)\n    total_tokens = len(doc)\n    if total_tokens == 0: return {}, 0\n    \n    counts = Counter([token.pos_ for token in doc])\n    \n    ratios = {\n        \"Noun Ratio\": counts.get(\"NOUN\", 0) / total_tokens,\n        \"Verb Ratio\": counts.get(\"VERB\", 0) / total_tokens,\n        \"Adjective Ratio\": counts.get(\"ADJ\", 0) / total_tokens,\n        \"Pronoun Ratio\": counts.get(\"PRON\", 0) / total_tokens,\n    }\n    \n    probs = np.array([c / total_tokens for c in counts.values()])\n    entropy = -(probs * np.log2(probs)).sum()\n    \n    return ratios, entropy\n\ndef detect_ai_human(text):\n    if not text.strip():\n        return \"Please enter text.\", {}, {}, \"N/A\"\n\n    ppl = calculate_perplexity(text)\n    ratios, entropy = analyze_pos_features(text)\n    \n    # Heuristic Scoring\n    ai_score = 0\n    human_score = 0\n    \n    # Perplexity Logic\n    if ppl < 20: ai_score += 4\n    elif ppl < 30: ai_score += 2\n    elif ppl > 45: human_score += 4\n    elif ppl > 35: human_score += 2\n    \n    # Entropy Logic\n    if entropy < 3.32: ai_score += 1\n    elif entropy > 3.36: human_score += 1\n    \n    # Ratio Logic\n    if ratios[\"Noun Ratio\"] > 0.22: ai_score += 1\n    if ratios[\"Verb Ratio\"] > 0.12: human_score += 1\n    \n    if ai_score > human_score: verdict = \"Likely AI \"\n    elif human_score > ai_score: verdict = \"Likely Human \"\n    else: verdict = \"Uncertain \"\n\n    return verdict, f\"{ppl:.2f}\", f\"{entropy:.4f}\", ratios","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CORRECTED GRADIO UI CODE ---\nimport gradio as gr\n\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# üïµÔ∏è AI vs Human Text Analyzer\")\n    gr.Markdown(\"Server running on Kaggle T4 GPU.\")\n    \n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\", lines=8, placeholder=\"Type here...\")\n            analyze_btn = gr.Button(\"Analyze\", variant=\"primary\")\n        \n        with gr.Column():\n            lbl_verdict = gr.Label(label=\"Verdict\")\n            with gr.Row():\n                # We define the label variable here as lbl_ppl\n                lbl_ppl = gr.Textbox(label=\"Perplexity\") \n                lbl_entropy = gr.Textbox(label=\"Entropy\")\n            lbl_ratios = gr.JSON(label=\"POS Ratios\")\n\n    analyze_btn.click(\n        fn=detect_ai_human,\n        inputs=input_text,\n        # CORRECTED LINE BELOW: used 'lbl_ppl' instead of 'ppl'\n        outputs=[lbl_verdict, lbl_ppl, lbl_entropy, lbl_ratios]\n    )\n\n# Launch with server share enabled\ndemo.launch(share=True, debug=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-----","metadata":{}},{"cell_type":"code","source":"import torch\nimport math\nimport gradio as gr\nimport spacy\nimport pandas as pd\nimport numpy as np\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast, BertTokenizer, BertModel\nfrom collections import Counter\nimport torch.nn as nn\nimport re\nimport unicodedata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:40:35.614224Z","iopub.execute_input":"2025-11-28T09:40:35.614554Z","iopub.status.idle":"2025-11-28T09:41:22.476209Z","shell.execute_reply.started":"2025-11-28T09:40:35.61453Z","shell.execute_reply":"2025-11-28T09:41:22.474991Z"}},"outputs":[{"name":"stderr","text":"2025-11-28 09:40:57.231397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764322857.461171     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764322857.524001     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- MODEL SETUP ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚è≥ Loading models on {device}...\")\n\ntry:\n    # Load GPT-2 for Perplexity\n    model_id = \"gpt2\"\n    gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n    gpt2_model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n    gpt2_model.eval()\n\n    # Load spaCy for POS\n    nlp = spacy.load(\"en_core_web_sm\")\n    \n    # Load BERT tokenizer and define model class\n    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n    \n    class BERTClassifier(nn.Module):\n        def __init__(self, model_name):\n            super(BERTClassifier, self).__init__()\n            self.bert = BertModel.from_pretrained(model_name)\n            self.drop = nn.Dropout(p=0.3)\n            self.out = nn.Linear(in_features=768, out_features=2)\n            \n        def forward(self, input_ids, attention_mask):\n            output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n            pooled_output = output[1]\n            output = self.drop(pooled_output)\n            return self.out(output)\n    \n    # Initialize BERT model and load trained weights\n    bert_model = BERTClassifier('bert-base-cased').to(device)\n    bert_model.load_state_dict(torch.load('/kaggle/input/aigen-bertmodel/pytorch/default/1/bert_model_state.bin', map_location=device))\n    bert_model.eval()\n    \n    print(\"All models loaded successfully.\")\nexcept Exception as e:\n    print(f\" Error loading models: {e}\")\n    print(\"Try restarting the session if this persists.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:44:16.97029Z","iopub.execute_input":"2025-11-28T09:44:16.971258Z","iopub.status.idle":"2025-11-28T09:44:28.396791Z","shell.execute_reply.started":"2025-11-28T09:44:16.971226Z","shell.execute_reply":"2025-11-28T09:44:28.395571Z"}},"outputs":[{"name":"stdout","text":"‚è≥ Loading models on cpu...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"519978ff7e0b4cf1a63eb260f143487c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7c4ffbf1fa4dbeb835e2952ab1bb45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e29aecf6cd04bf789687e27888b2c10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebb8a42f34bb4ab5a7c320080f51122a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab80cd045ee34aacac0bfbd3ed5efa27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f54ccceec14224ba62ad839c5fa4fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4256001889a432fb85dcd551202ce15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ac8e9a33b7429e9313fed105649e7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdaf8f65687c4e53bfe5d22114cbbf7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0771d00a44144abc8f3e2838f1d6ef70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f18ab7b8986147968b80f5304575a035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cedbe0805334acdb0a0887771c5a866"}},"metadata":{}},{"name":"stdout","text":"All models loaded successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- TEXT PREPROCESSING ---\ndef preprocess_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    \n    text = unicodedata.normalize('NFKC', text)\n    text = re.sub(r'<.*?>', '', text)\n    text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n    text = text.replace('\\\\', '')\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:44:45.935565Z","iopub.execute_input":"2025-11-28T09:44:45.935969Z","iopub.status.idle":"2025-11-28T09:44:45.942001Z","shell.execute_reply.started":"2025-11-28T09:44:45.935939Z","shell.execute_reply":"2025-11-28T09:44:45.940729Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# --- BERT PREDICTION ---\ndef predict_with_bert(text, max_len=512):\n    \"\"\"Returns probability that text is AI-generated\"\"\"\n    if not text.strip():\n        return 0.5, \"No text provided\"\n    \n    cleaned_text = preprocess_text(text)\n    \n    encoding = bert_tokenizer.encode_plus(\n        cleaned_text,\n        add_special_tokens=True,\n        max_length=max_len,\n        return_token_type_ids=False,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt',\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n        probs = torch.softmax(outputs, dim=1)\n        ai_prob = probs[0][1].item()  # Probability of AI class\n        \n    prediction = \"AI-generated\" if ai_prob > 0.5 else \"Human-written\"\n    confidence = max(ai_prob, 1 - ai_prob) * 100\n    \n    return ai_prob, f\"{prediction} ({confidence:.1f}% confidence)\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:46:16.508525Z","iopub.execute_input":"2025-11-28T09:46:16.50889Z","iopub.status.idle":"2025-11-28T09:46:16.516992Z","shell.execute_reply.started":"2025-11-28T09:46:16.508862Z","shell.execute_reply":"2025-11-28T09:46:16.515958Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# --- PERPLEXITY CALCULATION ---\ndef calculate_perplexity(text):\n    if not text or len(text.strip()) == 0:\n        return 0\n        \n    encodings = gpt2_tokenizer(text, return_tensors=\"pt\")\n    max_length = gpt2_model.config.n_positions\n    stride = 512\n    seq_len = encodings.input_ids.size(1)\n\n    nlls = []\n    prev_end_loc = 0\n    \n    for begin_loc in range(0, seq_len, stride):\n        end_loc = min(begin_loc + max_length, seq_len)\n        trg_len = end_loc - prev_end_loc \n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n\n        with torch.no_grad():\n            outputs = gpt2_model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs.loss\n\n        nlls.append(neg_log_likelihood)\n        prev_end_loc = end_loc\n        if end_loc == seq_len:\n            break\n\n    if not nlls: \n        return 0\n    ppl = torch.exp(torch.stack(nlls).mean())\n    return ppl.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:46:33.927011Z","iopub.execute_input":"2025-11-28T09:46:33.927994Z","iopub.status.idle":"2025-11-28T09:46:33.935638Z","shell.execute_reply.started":"2025-11-28T09:46:33.927963Z","shell.execute_reply":"2025-11-28T09:46:33.934743Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- POS ANALYSIS ---\ndef analyze_pos_features(text):\n    doc = nlp(text)\n    total_tokens = len(doc)\n    if total_tokens == 0: \n        return {}, 0\n    \n    counts = Counter([token.pos_ for token in doc])\n    \n    ratios = {\n        \"Noun Ratio\": counts.get(\"NOUN\", 0) / total_tokens,\n        \"Verb Ratio\": counts.get(\"VERB\", 0) / total_tokens,\n        \"Adjective Ratio\": counts.get(\"ADJ\", 0) / total_tokens,\n        \"Pronoun Ratio\": counts.get(\"PRON\", 0) / total_tokens,\n    }\n    \n    probs = np.array([c / total_tokens for c in counts.values()])\n    entropy = -(probs * np.log2(probs)).sum()\n    \n    return ratios, entropy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:47:22.725033Z","iopub.execute_input":"2025-11-28T09:47:22.725827Z","iopub.status.idle":"2025-11-28T09:47:22.732589Z","shell.execute_reply.started":"2025-11-28T09:47:22.725792Z","shell.execute_reply":"2025-11-28T09:47:22.731408Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def detect_ai_human(text):\n    if not text.strip():\n        return \"Please enter text.\", {}, {}, \"N/A\", \"N/A\", \"N/A\"\n\n    # BERT prediction\n    ai_prob, bert_verdict = predict_with_bert(text)\n    \n    # Perplexity\n    ppl = calculate_perplexity(text)\n    \n    # POS analysis\n    ratios, entropy = analyze_pos_features(text)\n    \n    # Heuristic scoring (from original code)\n    ai_score = 0\n    human_score = 0\n    \n    if ppl < 20: \n        ai_score += 4\n    elif ppl < 30: \n        ai_score += 2\n    elif ppl > 45: \n        human_score += 4\n    elif ppl > 35: \n        human_score += 2\n    \n    if entropy < 3.32: \n        ai_score += 1\n    elif entropy > 3.36: \n        human_score += 1\n    \n    if ratios[\"Noun Ratio\"] > 0.22: \n        ai_score += 1\n    if ratios[\"Verb Ratio\"] > 0.12: \n        human_score += 1\n    \n    if ai_score > human_score: \n        heuristic_verdict = \"Likely AI\"\n    elif human_score > ai_score: \n        heuristic_verdict = \"Likely Human\"\n    else: \n        heuristic_verdict = \"Uncertain\"\n\n    return bert_verdict, heuristic_verdict, f\"{ppl:.2f}\", f\"{entropy:.4f}\", f\"{ai_prob:.4f}\", ratios","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T09:47:55.274889Z","iopub.execute_input":"2025-11-28T09:47:55.275388Z","iopub.status.idle":"2025-11-28T09:47:55.282808Z","shell.execute_reply.started":"2025-11-28T09:47:55.275363Z","shell.execute_reply":"2025-11-28T09:47:55.281735Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- GRADIO UI ---\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# üïµÔ∏è AI vs Human Text Analyzer\")\n    gr.Markdown(\"Combines BERT classifier with perplexity and linguistic analysis. Running on Kaggle T4 GPU.\")\n    \n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\", lines=10, placeholder=\"Paste text here for analysis...\")\n            analyze_btn = gr.Button(\"üîç Analyze Text\", variant=\"primary\", size=\"lg\")\n        \n        with gr.Column():\n            gr.Markdown(\"### ü§ñ BERT Model Prediction\")\n            lbl_bert = gr.Textbox(label=\"BERT Verdict\", interactive=False)\n            lbl_bert_prob = gr.Textbox(label=\"AI Probability\", interactive=False)\n            \n            gr.Markdown(\"### üìä Heuristic Analysis\")\n            lbl_heuristic = gr.Textbox(label=\"Heuristic Verdict\", interactive=False)\n            \n            with gr.Row():\n                lbl_ppl = gr.Textbox(label=\"Perplexity\", interactive=False)\n                lbl_entropy = gr.Textbox(label=\"POS Entropy\", interactive=False)\n            \n            lbl_ratios = gr.JSON(label=\"Part-of-Speech Ratios\")\n\n    analyze_btn.click(\n        fn=detect_ai_human,\n        inputs=input_text,\n        outputs=[lbl_bert, lbl_heuristic, lbl_ppl, lbl_entropy, lbl_bert_prob, lbl_ratios]\n    )\n\n# Launch\ndemo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:30:20.116566Z","iopub.execute_input":"2025-11-28T10:30:20.117047Z","iopub.status.idle":"2025-11-28T11:00:42.350167Z","shell.execute_reply.started":"2025-11-28T10:30:20.117021Z","shell.execute_reply":"2025-11-28T11:00:42.348546Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://a91341d9b86c3a571e.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://a91341d9b86c3a571e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://a91341d9b86c3a571e.gradio.live\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Add this debug function to check what your model actually learned\ndef debug_bert_predictions(text):\n    cleaned_text = preprocess_text(text)\n    \n    encoding = bert_tokenizer.encode_plus(\n        cleaned_text,\n        add_special_tokens=True,\n        max_length=512,\n        return_token_type_ids=False,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt',\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n        probs = torch.softmax(outputs, dim=1)\n        \n        print(f\"Raw logits: {outputs[0]}\")\n        print(f\"Class 0 prob: {probs[0][0].item():.4f}\")\n        print(f\"Class 1 prob: {probs[0][1].item():.4f}\")\n        \n    return probs\n\n# Test with known human text\ndebug_bert_predictions(\"HIIIIIIIIIIIIII Its meeeeeeeeeee\")\n# Test with known AI text  \ndebug_bert_predictions(\"The implementation of artificial intelligence systems requires careful consideration.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:21:07.002102Z","iopub.execute_input":"2025-11-28T10:21:07.002884Z","iopub.status.idle":"2025-11-28T10:21:08.731155Z","shell.execute_reply.started":"2025-11-28T10:21:07.002855Z","shell.execute_reply":"2025-11-28T10:21:08.730169Z"}},"outputs":[{"name":"stdout","text":"Raw logits: tensor([-3.6124,  3.6595])\nClass 0 prob: 0.0007\nClass 1 prob: 0.9993\nRaw logits: tensor([-6.0712,  6.2176])\nClass 0 prob: 0.0000\nClass 1 prob: 1.0000\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"tensor([[4.6031e-06, 1.0000e+00]])"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# After loading, verify the weights aren't just random\nprint(\"First layer weight sample:\", bert_model.bert.embeddings.word_embeddings.weight[0][:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:20:26.978807Z","iopub.execute_input":"2025-11-28T10:20:26.979484Z","iopub.status.idle":"2025-11-28T10:20:26.988139Z","shell.execute_reply.started":"2025-11-28T10:20:26.979446Z","shell.execute_reply":"2025-11-28T10:20:26.987271Z"}},"outputs":[{"name":"stdout","text":"First layer weight sample: tensor([-0.0005, -0.0415,  0.0131,  0.0058, -0.0376], grad_fn=<SliceBackward0>)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Add detailed logging to compare preprocessing outputs\ndef debug_preprocessing(text):\n    print(f\"Original text length: {len(text)}\")\n    print(f\"Original first 100 chars: {text[:100]}\")\n    \n    cleaned = preprocess_text(text)\n    print(f\"Cleaned text length: {len(cleaned)}\")\n    print(f\"Cleaned first 100 chars: {cleaned[:100]}\")\n    \n    encoding = bert_tokenizer.encode_plus(\n        cleaned,\n        add_special_tokens=True,\n        max_length=512,\n        return_token_type_ids=False,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt',\n    )\n    \n    print(f\"Token IDs first 20: {encoding['input_ids'][0][:20]}\")\n    print(f\"Attention mask sum: {encoding['attention_mask'].sum()}\")\n    \n    return encoding\n\n# Test with a sample\ntest_text = \"This is a test sentence to check preprocessing.\"\ndebug_preprocessing(test_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:00:42.352078Z","iopub.execute_input":"2025-11-28T11:00:42.352783Z","iopub.status.idle":"2025-11-28T11:00:42.372754Z","shell.execute_reply.started":"2025-11-28T11:00:42.352754Z","shell.execute_reply":"2025-11-28T11:00:42.37179Z"}},"outputs":[{"name":"stdout","text":"Original text length: 47\nOriginal first 100 chars: This is a test sentence to check preprocessing.\nCleaned text length: 47\nCleaned first 100 chars: This is a test sentence to check preprocessing.\nToken IDs first 20: tensor([  101,  1188,  1110,   170,  2774,  5650,  1106,  4031,  3073,  1643,\n         2180, 22371,  1158,   119,   102,     0,     0,     0,     0,     0])\nAttention mask sum: 15\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  1188,  1110,   170,  2774,  5650,  1106,  4031,  3073,  1643,\n          2180, 22371,  1158,   119,   102,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# After loading the model\nbert_model.eval()\n\n# Disable all dropout layers explicitly\nfor module in bert_model.modules():\n    if isinstance(module, nn.Dropout):\n        module.p = 0.0\n        module.training = False\n\n# Disable gradient computation globally\ntorch.set_grad_enabled(False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:02:21.139737Z","iopub.execute_input":"2025-11-28T11:02:21.140095Z","iopub.status.idle":"2025-11-28T11:02:21.149971Z","shell.execute_reply.started":"2025-11-28T11:02:21.140068Z","shell.execute_reply":"2025-11-28T11:02:21.149065Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<torch.autograd.grad_mode.set_grad_enabled at 0x7a4d760fc390>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertTokenizer\nimport unicodedata\nimport re\n\n# Preprocessing function\ndef preprocess_text(text):\n    text = unicodedata.normalize('NFKC', text)\n    text = re.sub(r'<[^>]+>', '', text)\n    text = text.replace('\\n', ' ').replace('\\r', ' ')\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Model architecture\nclass BERTClassifier(nn.Module):\n    def __init__(self, n_classes=2):\n        super(BERTClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-cased')\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    \n    def forward(self, input_ids, attention_mask):\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = output[1]\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# Initialize\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load tokenizer FIRST (before any model operations)\nprint(\"Loading tokenizer...\")\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\n# Load model\nprint(\"Loading model...\")\nbert_model = BERTClassifier(n_classes=2)\nbert_model.load_state_dict(torch.load(\n    '/kaggle/input/aigen-bertmodel/pytorch/default/1/bert_model_state.bin',\n    map_location=device\n))\nbert_model = bert_model.to(device)\nbert_model.eval()\n\n# Disable gradients\ntorch.set_grad_enabled(False)\n\nprint(\"Model loaded successfully!\\n\")\n\n# Test samples\nhuman_text = \"\"\"In the artical Car Free Cities people all over the world are going carless. In German suburbs life goes on without cars.\"\"\"\n\nai_text = \"\"\"Climate change has become a hot topic in recent years, and many people are starting to realize the importance of taking action.\"\"\"\n\ndef test_sample(text, sample_name, expected_label):\n    print(f\"\\n{'='*60}\")\n    print(f\"Testing: {sample_name} (Expected: Label {expected_label})\")\n    print(f\"{'='*60}\")\n    \n    # Preprocess and tokenize\n    cleaned = preprocess_text(text)\n    encoding = bert_tokenizer.encode_plus(\n        cleaned,\n        add_special_tokens=True,\n        max_length=512,\n        return_token_type_ids=False,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt',\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    # Predict\n    outputs = bert_model(input_ids, attention_mask)\n    probs = torch.softmax(outputs, dim=1)\n    \n    class_0_prob = probs[0][0].item()\n    class_1_prob = probs[0][1].item()\n    predicted = torch.argmax(probs).item()\n    \n    print(f\"Class 0 probability: {class_0_prob:.4f} ({class_0_prob*100:.2f}%)\")\n    print(f\"Class 1 probability: {class_1_prob:.4f} ({class_1_prob*100:.2f}%)\")\n    print(f\"Predicted class: {predicted}\")\n    print(f\"Match: {'‚úì CORRECT' if predicted == expected_label else '‚úó WRONG'}\")\n    \n    return predicted == expected_label\n\n# Run tests\nresult1 = test_sample(human_text, \"HUMAN (shortened)\", expected_label=0)\nresult2 = test_sample(ai_text, \"AI (shortened)\", expected_label=1)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"SUMMARY: {sum([result1, result2])}/2 correct\")\nprint(f\"{'='*60}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:59:19.464926Z","iopub.execute_input":"2025-11-28T12:59:19.465312Z","iopub.status.idle":"2025-11-28T12:59:47.267913Z","shell.execute_reply.started":"2025-11-28T12:59:19.465286Z","shell.execute_reply":"2025-11-28T12:59:47.266915Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"Exception ignored in: <function _xla_gc_callback at 0x793bf991c7c0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n    def _xla_gc_callback(*args):\n    \nKeyboardInterrupt: \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Using device: cpu\nLoading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f57aecf06438480d935a771a1f9898bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88b85b287ed741b29fe4f577e6b37fb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde09318843543fb8665d73fe5ff8c53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12bfc4ffd3b9416fb59cab9b6677de18"}},"metadata":{}},{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e5a64d7200466289e8c888a98d4425"}},"metadata":{}},{"name":"stdout","text":"Model loaded successfully!\n\n\n============================================================\nTesting: HUMAN (shortened) (Expected: Label 0)\n============================================================\nClass 0 probability: 1.0000 (100.00%)\nClass 1 probability: 0.0000 (0.00%)\nPredicted class: 0\nMatch: ‚úì CORRECT\n\n============================================================\nTesting: AI (shortened) (Expected: Label 1)\n============================================================\nClass 0 probability: 0.0000 (0.00%)\nClass 1 probability: 1.0000 (100.00%)\nPredicted class: 1\nMatch: ‚úì CORRECT\n\n============================================================\nSUMMARY: 2/2 correct\n============================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}