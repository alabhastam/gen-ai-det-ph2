{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T19:39:11.133548Z","iopub.execute_input":"2025-10-27T19:39:11.133798Z","iopub.status.idle":"2025-10-27T19:39:13.574106Z","shell.execute_reply.started":"2025-10-27T19:39:11.133776Z","shell.execute_reply":"2025-10-27T19:39:13.572944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set a professional style for plots\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.dpi'] = 100 # Improve plot resolution\n\n# Load the dataset\n# Please adjust the file_path to where you have saved the CSV file\nfile_path = '/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv' \ntry:\n    df = pd.read_csv(file_path)\n    print(\"Dataset loaded successfully.\")\nexcept FileNotFoundError:\n    print(f\"Error: File not found at '{file_path}'. Please check the path.\")\n    # Create an empty DataFrame to prevent subsequent code from crashing\n    df = pd.DataFrame()\n\nif not df.empty:\n    # 1. Display the first 5 rows to understand the structure\n    print(\"\\n--- 1. First 5 Rows of the Dataset ---\")\n    display(df.head())\n\n    # 2. Get general information about the DataFrame (entry count, columns, non-null values, dtypes)\n    print(\"\\n\\n--- 2. DataFrame General Information ---\")\n    df.info()\n\n    # 3. Get descriptive statistics for numerical columns (in this case, 'label')\n    print(\"\\n\\n--- 3. Descriptive Statistics ---\")\n    display(df.describe())\n\n    # 4. Check for missing values in each column\n    print(\"\\n\\n--- 4. Checking for Missing Values ---\")\n    missing_values = df.isnull().sum()\n    if missing_values.sum() == 0:\n        print(\"No missing values found in the dataset.\")\n    else:\n        print(missing_values[missing_values > 0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T19:49:37.968373Z","iopub.execute_input":"2025-10-27T19:49:37.968839Z","iopub.status.idle":"2025-10-27T19:49:53.388691Z","shell.execute_reply.started":"2025-10-27T19:49:37.968796Z","shell.execute_reply":"2025-10-27T19:49:53.387263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not df.empty:\n    plt.figure(figsize=(10, 6))\n    \n    # Create a count plot for the label distribution\n    ax = sns.countplot(x='label', data=df, palette=['#4374B3', '#FF6347'])\n    \n    plt.title('Distribution of Human-Written vs. AI-Generated Essays', fontsize=16)\n    plt.xlabel('Label', fontsize=12)\n    plt.ylabel('Count', fontsize=12)\n    plt.xticks(ticks=[0, 1], labels=['Human (0)', 'AI-Generated (1)'])\n    \n    # Annotate the bars with the exact counts\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height():,}', \n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha='center', va='center', \n                    xytext=(0, 9), \n                    textcoords='offset points',\n                    fontsize=11)\n    \n    plt.show()\n\n    # Calculate the percentage of each class\n    label_counts_perc = df['label'].value_counts(normalize=True) * 100\n    print(\"\\nPercentage Distribution of Labels:\")\n    print(f\"Human-written essays (0): {label_counts_perc[0]:.2f}%\")\n    print(f\"AI-generated essays (1): {label_counts_perc[1]:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T19:52:19.222179Z","iopub.execute_input":"2025-10-27T19:52:19.222521Z","iopub.status.idle":"2025-10-27T19:52:19.593440Z","shell.execute_reply.started":"2025-10-27T19:52:19.222497Z","shell.execute_reply":"2025-10-27T19:52:19.592164Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- we do not have many problems using this dataset in case of data imbalance. We do not need any kind of oversampling tech in this project(that what I think now, maybe later we fix that)","metadata":{}},{"cell_type":"code","source":"if not df.empty:\n    plt.figure(figsize=(12, 10))\n    \n    # Plot the distribution of data sources\n    source_order = df['source'].value_counts().index\n    ax = sns.countplot(y='source', data=df, order=source_order, palette='viridis')\n    \n    plt.title('Distribution of Essays by Source', fontsize=16)\n    plt.xlabel('Count (Log Scale)', fontsize=12)\n    plt.ylabel('Source', fontsize=12)\n    plt.xscale('log') # Use a log scale due to the large variance in counts\n    \n    # Add count labels to the bars\n    for i, (p, count) in enumerate(zip(ax.patches, df['source'].value_counts(sort=True))):\n        ax.text(p.get_width() * 1.1, i, f'{count:,}', va='center')\n        \n    plt.show()\n\n    print(\"\\nEssay Counts from Each Source:\")\n    print(df['source'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T19:54:50.843598Z","iopub.execute_input":"2025-10-27T19:54:50.843926Z","iopub.status.idle":"2025-10-27T19:54:51.574707Z","shell.execute_reply.started":"2025-10-27T19:54:50.843904Z","shell.execute_reply":"2025-10-27T19:54:51.573788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not df.empty:\n    # Calculate word count for each essay\n    df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n\n    plt.figure(figsize=(12, 7))\n    \n    # Plot the distribution of word counts for each class\n    sns.histplot(data=df, x='word_count', hue='label', kde=True, bins=50, palette=['#4374B3', '#FF6347'])\n    \n    plt.title('Comparison of Essay Length Distribution (Word Count)', fontsize=16)\n    plt.xlabel('Word Count', fontsize=12)\n    plt.ylabel('Frequency', fontsize=12)\n    \n    # Manually create a legend\n    handles, _ = plt.gca().get_legend_handles_labels()\n    plt.legend(handles, ['AI-Generated (1)', 'Human (0)'], title='Label')\n\n    plt.xlim(0, 1500) # Limit x-axis for better visualization\n    plt.show()\n\n    # Show descriptive statistics of word count grouped by label\n    print(\"\\nDescriptive Statistics for Word Count by Label:\")\n    display(df.groupby('label')['word_count'].describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T19:56:23.148165Z","iopub.execute_input":"2025-10-27T19:56:23.148513Z","iopub.status.idle":"2025-10-27T19:56:25.140556Z","shell.execute_reply.started":"2025-10-27T19:56:23.148487Z","shell.execute_reply":"2025-10-27T19:56:25.139768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not df.empty:\n    # Due to the large number of unique prompts, let's visualize the top 15\n    top_n_prompts = 15\n    prompt_counts = df['prompt_name'].value_counts()\n    \n    plt.figure(figsize=(12, 8))\n    sns.barplot(y=prompt_counts.index[:top_n_prompts], x=prompt_counts.values[:top_n_prompts], orient='h', palette='plasma')\n    \n    plt.title(f'Top {top_n_prompts} Most Frequent Essay Prompts', fontsize=16)\n    plt.xlabel('Count', fontsize=12)\n    plt.ylabel('Prompt Name', fontsize=12)\n    plt.show()\n\n    print(f\"\\nTotal number of unique prompts: {df['prompt_name'].nunique()}\")\n    print(f\"\\nTop {top_n_prompts} most frequent prompts:\")\n    print(prompt_counts.head(top_n_prompts))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:01:33.966751Z","iopub.execute_input":"2025-10-27T20:01:33.967276Z","iopub.status.idle":"2025-10-27T20:01:34.406566Z","shell.execute_reply.started":"2025-10-27T20:01:33.967106Z","shell.execute_reply":"2025-10-27T20:01:34.404396Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- If a specific prompt_name (topic) is exclusively or heavily associated with only one label (AI or Human), the model might learn to use the topic as a shortcut for prediction.\n- For example, if every essay about “Driverless cars” is AI-generated, your model might learn:\n\nif text contains \"driverless car\", \"autonomous vehicle\", \"self-driving\" -> predict AI (1)\n\nThis model would fail spectacularly if it ever encountered a human-written essay on the same topic. Let’s systematically investigate this potential leakage.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# We assume 'df' is already loaded from the previous steps\nif not df.empty:\n    print(\"--- Analyzing Label Distribution per Prompt Name ---\")\n    \n    # fill_value=0 handles prompts that may only have one label type\n    prompt_label_dist = df.groupby('prompt_name')['label'].value_counts().unstack(fill_value=0)\n    \n    # Rename columns for clarity\n    prompt_label_dist = prompt_label_dist.rename(columns={0: 'human_count', 1: 'ai_count'})\n\n    # Calculate the total count and the percentage of AI-generated essays for each prompt\n    prompt_label_dist['total_count'] = prompt_label_dist['human_count'] + prompt_label_dist['ai_count']\n    prompt_label_dist['ai_percentage'] = (prompt_label_dist['ai_count'] / prompt_label_dist['total_count']) * 100\n\n    # Sort by the AI percentage to easily find the most skewed prompts\n    prompt_label_dist_sorted = prompt_label_dist.sort_values(by='ai_percentage', ascending=False)\n\n    print(\"\\n--- Distribution of AI vs. Human Labels within each Prompt ---\")\n    print(\"Prompts are sorted by the percentage of AI-generated essays (from highest to lowest).\")\n    display(prompt_label_dist_sorted)\n\n    # --- Identifying \"Leaky\" Prompts ---\n    # Prompts that are 100% AI or 100% Human\n    pure_ai_prompts = prompt_label_dist_sorted[prompt_label_dist_sorted['ai_percentage'] == 100]\n    pure_human_prompts = prompt_label_dist_sorted[prompt_label_dist_sorted['ai_percentage'] == 0]\n\n    # Prompts that are heavily skewed (e.g., > 95% AI or < 5% AI)\n    highly_skewed_prompts = prompt_label_dist_sorted[\n        (prompt_label_dist_sorted['ai_percentage'] > 95) | \n        (prompt_label_dist_sorted['ai_percentage'] < 5)\n    ]\n\n    print(f\"\\n\\n--- Leakage Investigation Results ---\")\n    print(f\"Total number of unique prompts: {len(prompt_label_dist)}\")\n    print(f\"Number of prompts that are 100% AI-generated: {len(pure_ai_prompts)}\")\n    print(f\"Number of prompts that are 100% Human-written: {len(pure_human_prompts)}\")\n    print(f\"Total number of 'pure' (100% one label) or highly skewed (<5% or >95%) prompts: {len(highly_skewed_prompts)}\")\n\n    if len(pure_ai_prompts) > 0:\n        print(\"\\nPrompts that are 100% AI-Generated (High Risk of Leakage):\")\n        display(pure_ai_prompts.head()) # Display first few\n    \n    if len(pure_human_prompts) > 0:\n        print(\"\\nPrompts that are 100% Human-Written (High Risk of Leakage):\")\n        display(pure_human_prompts.head()) # Display first few\n\n    # --- Visualization ---\n    plt.figure(figsize=(12, 8))\n    sns.histplot(prompt_label_dist['ai_percentage'], bins=20, kde=False)\n    plt.title('Histogram of AI-Generated Percentage Across All Prompts', fontsize=16)\n    plt.xlabel('Percentage of Essays that are AI-Generated (%)', fontsize=12)\n    plt.ylabel('Number of Prompts', fontsize=12)\n    plt.axvline(50, color='r', linestyle='--', label='50% (Balanced)')\n    plt.axvline(95, color='orange', linestyle=':', label='95% Skew')\n    plt.axvline(5, color='orange', linestyle=':', label='5% Skew')\n    plt.legend()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T20:07:17.570077Z","iopub.execute_input":"2025-10-27T20:07:17.570419Z","iopub.status.idle":"2025-10-27T20:07:17.982905Z","shell.execute_reply.started":"2025-10-27T20:07:17.570397Z","shell.execute_reply":"2025-10-27T20:07:17.981764Z"}},"outputs":[],"execution_count":null}]}