{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff79e05a",
   "metadata": {
    "papermill": {
     "duration": 0.002391,
     "end_time": "2025-11-13T10:50:01.090556",
     "exception": false,
     "start_time": "2025-11-13T10:50:01.088165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"\n",
    "    background-color:#1e1e1e;\n",
    "    color:#f5f5f5;\n",
    "    padding:35px;\n",
    "    border-radius:12px;\n",
    "    font-family:'Segoe UI', 'Helvetica Neue', sans-serif;\n",
    "    line-height:1.7;\n",
    "    font-size:15px;\n",
    "\">\n",
    "\n",
    "<h1 style=\"text-align:center; color:#ffc107;\">üß© Understanding Perplexity in Language Models</h1>\n",
    "\n",
    "<p>\n",
    "<b>Perplexity (PP)</b> quantifies how confidently a language model predicts the next token.  \n",
    "It‚Äôs essentially the model‚Äôs <i>‚Äúdegree of surprise.‚Äù</i>  \n",
    "A low perplexity means smooth, natural predictions ‚Äî a high one means confusion or uncertainty.\n",
    "</p>\n",
    "\n",
    "<hr style=\"border:0.5px solid #444;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üìò Mathematical Definition</h2>\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "PP = exp(H) = exp( ‚àí (1/N) Œ£ log P(w<sub>i</sub> | w<sub>1</sub>, ..., w<sub>i‚àí1</sub>) )\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "  <li><b>P(w<sub>i</sub> | w<sub>1</sub>, ..., w<sub>i‚àí1</sub>)</b>: Probability of the next token.</li>\n",
    "  <li><b>N</b>: Number of words in the sequence.</li>\n",
    "  <li><b>log‚ÄØP</b>: Log-likelihood, later exponentiated to get back to normal scale.</li>\n",
    "</ul>\n",
    "\n",
    "<p><i>Lower PP ‚Üí more predictable and fluent text.<br>Higher PP ‚Üí randomness or topic drift.</i></p>\n",
    "\n",
    "<hr style=\"border:0.5px solid #444;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üìä Numerical Interpretation</h2>\n",
    "\n",
    "<table style=\"width:100%; border-collapse:collapse; margin-top:8px;\">\n",
    "<thead>\n",
    "<tr style=\"background-color:#2a2a2a; color:#ffd54f;\">\n",
    "  <th style=\"padding:8px; border-bottom:1px solid #444;\">Perplexity Range</th>\n",
    "  <th style=\"padding:8px; border-bottom:1px solid #444;\">Meaning</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "  <td style=\"padding:8px; border-bottom:1px solid #333;\">10 ‚Äì 25</td>\n",
    "  <td style=\"padding:8px; border-bottom:1px solid #333;\">Excellent ‚Äî coherent and human‚Äëlike fluency.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td style=\"padding:8px; border-bottom:1px solid #333;\">30 ‚Äì 70</td>\n",
    "  <td style=\"padding:8px; border-bottom:1px solid #333;\">Moderate ‚Äî the model struggles slightly with context.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td style=\"padding:8px; border-bottom:1px solid #333;\">&gt; 100</td>\n",
    "  <td style=\"padding:8px; border-bottom:1px solid #333;\">Poor ‚Äî the model is confused or off‚Äëdomain.</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<hr style=\"border:0.5px solid #444;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">‚öôÔ∏è Use Cases in NLP</h2>\n",
    "\n",
    "<ol>\n",
    "<li><b>Model Evaluation:</b> Compare language models ‚Äî the lower the PP, the better.</li>\n",
    "<li><b>AI vs Human Detection:</b> AI text = low & stable PP. Human text = high & variable PP.</li>\n",
    "<li><b>Dataset Filtering:</b> Identify unnatural or corrupted samples during preprocessing.</li>\n",
    "</ol>\n",
    "\n",
    "<hr style=\"border:0.5px solid #444;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üß† Intuitive Example</h2>\n",
    "\n",
    "<p>If a model predicts the next word in <code style=\"background-color:#2f2f2f; color:#fff;\">I love ___</code>:</p>\n",
    "\n",
    "<ul>\n",
    "<li><b>‚Äúyou‚Äù</b> with probability 0.9 ‚Üí low PP (expected, fluent).</li>\n",
    "<li><b>‚Äúbananas‚Äù</b> with probability 0.01 ‚Üí high PP (unexpected, off‚Äëcontext).</li>\n",
    "</ul>\n",
    "\n",
    "<hr style=\"border:0.5px solid #444;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üêç Example in Python (Kaggle‚ÄëReady)</h2>\n",
    "\n",
    "<pre style=\"background-color:#2b2b2b; color:#e0e0e0; padding:14px; border-radius:8px; overflow-x:auto;\">\n",
    "<code class=\"language-python\">\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch, math\n",
    "\n",
    "# Load GPT-2\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Example text\n",
    "text = \"I love you so much.\"\n",
    "\n",
    "# Encode & evaluate\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
    "perplexity = math.exp(loss)\n",
    "\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "</code>\n",
    "</pre>\n",
    "\n",
    "<hr style=\"border:0.5px solid #444;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üß© Summary</h2>\n",
    "\n",
    "<ul>\n",
    "<li><b>Definition:</b> Statistical indicator of prediction confidence.</li>\n",
    "<li><b>Goal:</b> Lower perplexity = stronger model quality.</li>\n",
    "<li><b>AI Detection Insight:</b> Human writing ‚Üí variable PP; AI ‚Üí smooth & uniform PP.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a93989",
   "metadata": {
    "papermill": {
     "duration": 0.001703,
     "end_time": "2025-11-13T10:50:01.094064",
     "exception": false,
     "start_time": "2025-11-13T10:50:01.092361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"\n",
    "    background-color:#1c1c1c;\n",
    "    color:#f5f5f5;\n",
    "    padding:40px;\n",
    "    border-radius:12px;\n",
    "    font-family:'Segoe UI','Helvetica Neue',sans-serif;\n",
    "    line-height:1.8;\n",
    "    font-size:15px;\n",
    "\">\n",
    "\n",
    "<h1 style=\"text-align:center; color:#ffca28;\">ü§ñ Perplexity as a Statistical Signal for AI‚ÄØvs‚ÄØHuman Text Detection</h1>\n",
    "\n",
    "<p>\n",
    "<b>Perplexity (PP)</b> is a central measurement in computational linguistics used to evaluate\n",
    "how well a language model predicts a given piece of text. In the context of \n",
    "<b>AI‚ÄØvs‚ÄØhuman text detection</b>, perplexity captures the difference between \n",
    "machine-generated fluency and human unpredictability. Because AI text is optimized to seem smooth and linguistically probable,\n",
    "it tends to yield <b>low, consistent perplexity</b>, whereas human text is more\n",
    "erratic, imaginative, and semantically diverse‚Äîresulting in <b>higher and more variable perplexity</b>.\n",
    "</p>\n",
    "\n",
    "<hr style=\"border:0.5px solid #333;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üîπ 1. Conceptual Foundation</h2>\n",
    "\n",
    "<p>\n",
    "Perplexity reflects the ‚Äúconfusion‚Äù of a language model when reading a sequence of words.\n",
    "Formally derived from the exponentiation of average negative log-likelihood, perplexity measures how \n",
    "uncertain the model is about the next token. \n",
    "A perfect model that predicts every token with full confidence would score a PP of <b>1</b>, \n",
    "while a poorly aligned model observing unexpected words shows a high PP value.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "When used for authorship classification:\n",
    "<ul>\n",
    "<li>AI text aims for <i>optimal predictability</i>; a model sees its own writing style as predictable.</li>\n",
    "<li>Human text is <i>less optimized</i>, embedding emotion, digression, and rare phrasing‚Äîfeatures that increase PP variance.</li>\n",
    "<li>Thus, analyzing perplexity distribution over segments (sentences or paragraphs) exposes the nature of authorship.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<hr style=\"border:0.5px solid #333;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üîπ 2. Behavioral Patterns in AI and Human Text</h2>\n",
    "\n",
    "<p>\n",
    "The following trends typically emerge when plotting perplexity values across sentences:\n",
    "</p>\n",
    "\n",
    "<table style=\"width:100%; border-collapse:collapse; margin-top:8px;\">\n",
    "<thead>\n",
    "<tr style=\"background-color:#2a2a2a; color:#ffd54f;\">\n",
    "<th style=\"padding:8px; border-bottom:1px solid #444;\">Metric</th>\n",
    "<th style=\"padding:8px; border-bottom:1px solid #444;\">AI‚ÄëGenerated Text</th>\n",
    "<th style=\"padding:8px; border-bottom:1px solid #444;\">Human‚ÄëWritten Text</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Average Perplexity</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Lower (‚âà‚ÄØ20‚Äì40)</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Higher (‚âà‚ÄØ60‚Äì120)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Variance</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Low, steady across sentences</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">High, with irregular fluctuations</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Local Peaks</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Flattened or absent</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Frequent and sharp</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Entropy Landscape</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Smooth, narrow entropy basin</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Rough, multi‚Äëmodal entropy surface</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<p>\n",
    "This statistical asymmetry forms the mathematical basis for AI detection:\n",
    "low average perplexity + low variance often signals automated generation,\n",
    "while irregular high‚Äëentropy traces often point to genuine human authorship.\n",
    "</p>\n",
    "\n",
    "<hr style=\"border:0.5px solid #333;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üîπ 3. Entropy Profile and Signal Extraction</h2>\n",
    "\n",
    "<p>\n",
    "Instead of relying on a single perplexity number, \n",
    "a more reliable indicator is the <b>perplexity profile</b>‚Äîthe sequence of PP values\n",
    "computed over multiple contiguous sentences.  \n",
    "From this curve, one can derive features such as:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "<li><b>Mean‚ÄØPP</b> ‚Äî overall predictability of the text.</li>\n",
    "<li><b>Standard‚ÄØDeviation‚ÄØPP</b> ‚Äî stylistic variability or creativity.</li>\n",
    "<li><b>Range‚ÄØPP</b> ‚Äî scope of change in linguistic entropy.</li>\n",
    "<li><b>Max‚ÄØJump‚ÄØPP</b> ‚Äî abruptness between consecutive PP values.</li>\n",
    "<li><b>Skewness‚ÄØPP</b> ‚Äî shape of the distribution, correlating with bursts of novelty.</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "Human authors often demonstrate broader range and higher standard deviation, \n",
    "while AI‚Äëgenerated sequences cluster tightly around a stable mean.\n",
    "Computing these derivative signals produces a rich \"entropy fingerprint\"\n",
    "that models can use for classification.\n",
    "</p>\n",
    "\n",
    "<hr style=\"border:0.5px solid #333;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üîπ 4. Interpretive Perspective: What Perplexity Reveals</h2>\n",
    "\n",
    "<ul>\n",
    "<li><b>Semantic Control:</b> AI has high local coherence; perplexity remains low.</li>\n",
    "<li><b>Idea Drift:</b> Humans allow narrative tangents or emotional flourish, creating peaks.</li>\n",
    "<li><b>Cognitive Noise:</b> Variability introduced by mood, bias, opinion, or ambiguity increases PP.</li>\n",
    "<li><b>Algorithmic Consistency:</b> A model‚Äôs decoding algorithm enforces uniform probability flow, lowering PP rise.</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "Thus, perplexity effectively measures <i>how mechanical</i> or <i>how inspired</i> a piece of writing appears.\n",
    "Even without stylistic analysis, its statistics alone can approximate author identity.\n",
    "</p>\n",
    "\n",
    "<hr style=\"border:0.5px solid #333;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üîπ 5. Hybrid Feature Integration</h2>\n",
    "\n",
    "<p>\n",
    "While perplexity itself is powerful, combining it with syntactic or emotional descriptors \n",
    "yields stronger performance in distinguishing humans from machines.\n",
    "Common hybrid features include:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "<li><b>Part‚Äëof‚ÄëSpeech ratios</b> ‚Äì measuring functional diversity of language.</li>\n",
    "<li><b>Lexical rarity</b> ‚Äì frequency of uncommon tokens relative to a corpus baseline.</li>\n",
    "<li><b>Sentiment entropy</b> ‚Äì emotional irregularity correlating with genuine tone shifts.</li>\n",
    "<li><b>Burstiness</b> ‚Äì uneven punctuation and clause density patterns found in creative writing.</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "Perplexity acts as the quantitative core: a signal of probability smoothness, while the others\n",
    "add linguistic nuance. Combined, they construct a robust multidimensional fingerprint for AI‚Äëvs‚Äëhuman classification.\n",
    "</p>\n",
    "\n",
    "<hr style=\"border:0.5px solid #333;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üîπ 6. Strengths and Limitations</h2>\n",
    "\n",
    "<table style=\"width:100%; border-collapse:collapse; margin-top:8px;\">\n",
    "<thead>\n",
    "<tr style=\"background-color:#2a2a2a; color:#ffd54f;\">\n",
    "<th style=\"padding:8px; border-bottom:1px solid #444;\">Aspect</th>\n",
    "<th style=\"padding:8px; border-bottom:1px solid #444;\">Advantages</th>\n",
    "<th style=\"padding:8px; border-bottom:1px solid #444;\">Limitations</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Interpretability</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Simple mathematical meaning ‚Äî lower is more predictable</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Requires reference LM calibrated to domain vocabulary</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Effectiveness</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Strong accuracy on raw LLM text spotting</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Less decisive on short or mixed texts</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Robustness</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Works even when style-masking features fail</td>\n",
    "<td style=\"padding:8px; border-bottom:1px solid #333;\">Advanced LLMs can simulate human‚Äëlike PP variability</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<hr style=\"border:0.5px solid #333;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üîπ 7. Practical Insights</h2>\n",
    "\n",
    "<p>\n",
    "In real detection pipelines (academic or industrial), perplexity features are extracted at the\n",
    "document or sentence level, standardized, and fed into classifiers such as\n",
    "Random Forests, SVMs, or neural discriminators.\n",
    "The interpretability of such systems stems from the fact that human creative irregularity\n",
    "cannot be easily replicated by deterministic sampling from neural networks‚Äî\n",
    "not yet, at least.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Perplexity thus functions as a <b>quantitative window into cognitive spontaneity</b>.\n",
    "Humans tend to surprise language models; language models rarely surprise themselves.\n",
    "</p>\n",
    "\n",
    "<hr style=\"border:0.5px solid #333;\">\n",
    "\n",
    "<h2 style=\"color:#ffb74d;\">üîπ 8. Summary</h2>\n",
    "\n",
    "<p>\n",
    "Perplexity transforms linguistic predictability into a diagnostic signal for authorship.\n",
    "Low, uniform perplexity trajectories reflect algorithmic continuity, while high and uneven\n",
    "profiles reveal human expressiveness.\n",
    "As AI text sophistication increases, future research focuses on modeling the\n",
    "<i>dynamics of perplexity variance</i> and blending it with semantic and emotional irregularity\n",
    "for reliable, explainable AI‚Äëvs‚Äëhuman classification.\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.426617,
   "end_time": "2025-11-13T10:50:01.516381",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-13T10:49:56.089764",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
