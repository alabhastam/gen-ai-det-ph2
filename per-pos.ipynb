{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"\n    background-color:#1e1e1e;\n    color:#f5f5f5;\n    padding:35px;\n    border-radius:12px;\n    font-family:'Segoe UI', 'Helvetica Neue', sans-serif;\n    line-height:1.7;\n    font-size:15px;\n\">\n\n<h1 style=\"text-align:center; color:#ffc107;\">üß© Understanding Perplexity in Language Models</h1>\n\n<p>\n<b>Perplexity (PP)</b> quantifies how confidently a language model predicts the next token.  \nIt‚Äôs essentially the model‚Äôs <i>‚Äúdegree of surprise.‚Äù</i>  \nA low perplexity means smooth, natural predictions ‚Äî a high one means confusion or uncertainty.\n</p>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">üìò Mathematical Definition</h2>\n\n<p style=\"text-align:center;\">\nPP = exp(H) = exp( ‚àí (1/N) Œ£ log P(w<sub>i</sub> | w<sub>1</sub>, ..., w<sub>i‚àí1</sub>) )\n</p>\n\n<ul>\n  <li><b>P(w<sub>i</sub> | w<sub>1</sub>, ..., w<sub>i‚àí1</sub>)</b>: Probability of the next token.</li>\n  <li><b>N</b>: Number of words in the sequence.</li>\n  <li><b>log‚ÄØP</b>: Log-likelihood, later exponentiated to get back to normal scale.</li>\n</ul>\n\n<p><i>Lower PP ‚Üí more predictable and fluent text.<br>Higher PP ‚Üí randomness or topic drift.</i></p>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">üìä Numerical Interpretation</h2>\n\n<table style=\"width:100%; border-collapse:collapse; margin-top:8px;\">\n<thead>\n<tr style=\"background-color:#2a2a2a; color:#ffd54f;\">\n  <th style=\"padding:8px; border-bottom:1px solid #444;\">Perplexity Range</th>\n  <th style=\"padding:8px; border-bottom:1px solid #444;\">Meaning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">10 ‚Äì 25</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Excellent ‚Äî coherent and human‚Äëlike fluency.</td>\n</tr>\n<tr>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">30 ‚Äì 70</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Moderate ‚Äî the model struggles slightly with context.</td>\n</tr>\n<tr>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">&gt; 100</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Poor ‚Äî the model is confused or off‚Äëdomain.</td>\n</tr>\n</tbody>\n</table>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">‚öôÔ∏è Use Cases in NLP</h2>\n\n<ol>\n<li><b>Model Evaluation:</b> Compare language models ‚Äî the lower the PP, the better.</li>\n<li><b>AI vs Human Detection:</b> AI text = low & stable PP. Human text = high & variable PP.</li>\n<li><b>Dataset Filtering:</b> Identify unnatural or corrupted samples during preprocessing.</li>\n</ol>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">üß† Intuitive Example</h2>\n\n<p>If a model predicts the next word in <code style=\"background-color:#2f2f2f; color:#fff;\">I love ___</code>:</p>\n\n<ul>\n<li><b>‚Äúyou‚Äù</b> with probability 0.9 ‚Üí low PP (expected, fluent).</li>\n<li><b>‚Äúbananas‚Äù</b> with probability 0.01 ‚Üí high PP (unexpected, off‚Äëcontext).</li>\n</ul>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">üêç Example in Python (Kaggle‚ÄëReady)</h2>\n\n<pre style=\"background-color:#2b2b2b; color:#e0e0e0; padding:14px; border-radius:8px; overflow-x:auto;\">\n<code class=\"language-python\">\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nimport torch, math\n\n# Load GPT-2\nmodel_name = \"gpt2\"\ntokenizer = GPT2TokenizerFast.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Example text\ntext = \"I love you so much.\"\n\n# Encode & evaluate\ninputs = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\nperplexity = math.exp(loss)\n\nprint(f\"Perplexity: {perplexity:.2f}\")\n</code>\n</pre>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">üß© Summary</h2>\n\n<ul>\n<li><b>Definition:</b> Statistical indicator of prediction confidence.</li>\n<li><b>Goal:</b> Lower perplexity = stronger model quality.</li>\n<li><b>AI Detection Insight:</b> Human writing ‚Üí variable PP; AI ‚Üí smooth & uniform PP.</li>\n</ul>\n\n\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"\n    background-color:#1c1c1c;\n    color:#f5f5f5;\n    padding:40px;\n    border-radius:12px;\n    font-family:'Segoe UI','Helvetica Neue',sans-serif;\n    line-height:1.8;\n    font-size:15px;\n\">\n\n<h1 style=\"text-align:center; color:#ffca28;\">ü§ñ Perplexity as a Statistical Signal for AI‚ÄØvs‚ÄØHuman Text Detection</h1>\n\n<p>\n<b>Perplexity (PP)</b> is a central measurement in computational linguistics used to evaluate\nhow well a language model predicts a given piece of text. In the context of \n<b>AI‚ÄØvs‚ÄØhuman text detection</b>, perplexity captures the difference between \nmachine-generated fluency and human unpredictability. Because AI text is optimized to seem smooth and linguistically probable,\nit tends to yield <b>low, consistent perplexity</b>, whereas human text is more\nerratic, imaginative, and semantically diverse‚Äîresulting in <b>higher and more variable perplexity</b>.\n</p>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 1. Conceptual Foundation</h2>\n\n<p>\nPerplexity reflects the ‚Äúconfusion‚Äù of a language model when reading a sequence of words.\nFormally derived from the exponentiation of average negative log-likelihood, perplexity measures how \nuncertain the model is about the next token. \nA perfect model that predicts every token with full confidence would score a PP of <b>1</b>, \nwhile a poorly aligned model observing unexpected words shows a high PP value.\n</p>\n\n<p>\nWhen used for authorship classification:\n<ul>\n<li>AI text aims for <i>optimal predictability</i>; a model sees its own writing style as predictable.</li>\n<li>Human text is <i>less optimized</i>, embedding emotion, digression, and rare phrasing‚Äîfeatures that increase PP variance.</li>\n<li>Thus, analyzing perplexity distribution over segments (sentences or paragraphs) exposes the nature of authorship.</li>\n</ul>\n</p>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 2. Behavioral Patterns in AI and Human Text</h2>\n\n<p>\nThe following trends typically emerge when plotting perplexity values across sentences:\n</p>\n\n<table style=\"width:100%; border-collapse:collapse; margin-top:8px;\">\n<thead>\n<tr style=\"background-color:#2a2a2a; color:#ffd54f;\">\n<th style=\"padding:8px; border-bottom:1px solid #444;\">Metric</th>\n<th style=\"padding:8px; border-bottom:1px solid #444;\">AI‚ÄëGenerated Text</th>\n<th style=\"padding:8px; border-bottom:1px solid #444;\">Human‚ÄëWritten Text</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Average Perplexity</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Lower (‚âà‚ÄØ20‚Äì40)</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Higher (‚âà‚ÄØ60‚Äì120)</td>\n</tr>\n<tr>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Variance</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Low, steady across sentences</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">High, with irregular fluctuations</td>\n</tr>\n<tr>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Local Peaks</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Flattened or absent</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Frequent and sharp</td>\n</tr>\n<tr>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Entropy Landscape</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Smooth, narrow entropy basin</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Rough, multi‚Äëmodal entropy surface</td>\n</tr>\n</tbody>\n</table>\n\n<p>\nThis statistical asymmetry forms the mathematical basis for AI detection:\nlow average perplexity + low variance often signals automated generation,\nwhile irregular high‚Äëentropy traces often point to genuine human authorship.\n</p>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 3. Entropy Profile and Signal Extraction</h2>\n\n<p>\nInstead of relying on a single perplexity number, \na more reliable indicator is the <b>perplexity profile</b>‚Äîthe sequence of PP values\ncomputed over multiple contiguous sentences.  \nFrom this curve, one can derive features such as:\n</p>\n\n<ul>\n<li><b>Mean‚ÄØPP</b> ‚Äî overall predictability of the text.</li>\n<li><b>Standard‚ÄØDeviation‚ÄØPP</b> ‚Äî stylistic variability or creativity.</li>\n<li><b>Range‚ÄØPP</b> ‚Äî scope of change in linguistic entropy.</li>\n<li><b>Max‚ÄØJump‚ÄØPP</b> ‚Äî abruptness between consecutive PP values.</li>\n<li><b>Skewness‚ÄØPP</b> ‚Äî shape of the distribution, correlating with bursts of novelty.</li>\n</ul>\n\n<p>\nHuman authors often demonstrate broader range and higher standard deviation, \nwhile AI‚Äëgenerated sequences cluster tightly around a stable mean.\nComputing these derivative signals produces a rich \"entropy fingerprint\"\nthat models can use for classification.\n</p>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 4. Interpretive Perspective: What Perplexity Reveals</h2>\n\n<ul>\n<li><b>Semantic Control:</b> AI has high local coherence; perplexity remains low.</li>\n<li><b>Idea Drift:</b> Humans allow narrative tangents or emotional flourish, creating peaks.</li>\n<li><b>Cognitive Noise:</b> Variability introduced by mood, bias, opinion, or ambiguity increases PP.</li>\n<li><b>Algorithmic Consistency:</b> A model‚Äôs decoding algorithm enforces uniform probability flow, lowering PP rise.</li>\n</ul>\n\n<p>\nThus, perplexity effectively measures <i>how mechanical</i> or <i>how inspired</i> a piece of writing appears.\nEven without stylistic analysis, its statistics alone can approximate author identity.\n</p>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 5. Hybrid Feature Integration</h2>\n\n<p>\nWhile perplexity itself is powerful, combining it with syntactic or emotional descriptors \nyields stronger performance in distinguishing humans from machines.\nCommon hybrid features include:\n</p>\n\n<ul>\n<li><b>Part‚Äëof‚ÄëSpeech ratios</b> ‚Äì measuring functional diversity of language.</li>\n<li><b>Lexical rarity</b> ‚Äì frequency of uncommon tokens relative to a corpus baseline.</li>\n<li><b>Sentiment entropy</b> ‚Äì emotional irregularity correlating with genuine tone shifts.</li>\n<li><b>Burstiness</b> ‚Äì uneven punctuation and clause density patterns found in creative writing.</li>\n</ul>\n\n<p>\nPerplexity acts as the quantitative core: a signal of probability smoothness, while the others\nadd linguistic nuance. Combined, they construct a robust multidimensional fingerprint for AI‚Äëvs‚Äëhuman classification.\n</p>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 6. Strengths and Limitations</h2>\n\n<table style=\"width:100%; border-collapse:collapse; margin-top:8px;\">\n<thead>\n<tr style=\"background-color:#2a2a2a; color:#ffd54f;\">\n<th style=\"padding:8px; border-bottom:1px solid #444;\">Aspect</th>\n<th style=\"padding:8px; border-bottom:1px solid #444;\">Advantages</th>\n<th style=\"padding:8px; border-bottom:1px solid #444;\">Limitations</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Interpretability</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Simple mathematical meaning ‚Äî lower is more predictable</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Requires reference LM calibrated to domain vocabulary</td>\n</tr>\n<tr>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Effectiveness</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Strong accuracy on raw LLM text spotting</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Less decisive on short or mixed texts</td>\n</tr>\n<tr>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Robustness</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Works even when style-masking features fail</td>\n<td style=\"padding:8px; border-bottom:1px solid #333;\">Advanced LLMs can simulate human‚Äëlike PP variability</td>\n</tr>\n</tbody>\n</table>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 7. Practical Insights</h2>\n\n<p>\nIn real detection pipelines (academic or industrial), perplexity features are extracted at the\ndocument or sentence level, standardized, and fed into classifiers such as\nRandom Forests, SVMs, or neural discriminators.\nThe interpretability of such systems stems from the fact that human creative irregularity\ncannot be easily replicated by deterministic sampling from neural networks‚Äî\nnot yet, at least.\n</p>\n\n<p>\nPerplexity thus functions as a <b>quantitative window into cognitive spontaneity</b>.\nHumans tend to surprise language models; language models rarely surprise themselves.\n</p>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 8. Summary</h2>\n\n<p>\nPerplexity transforms linguistic predictability into a diagnostic signal for authorship.\nLow, uniform perplexity trajectories reflect algorithmic continuity, while high and uneven\nprofiles reveal human expressiveness.\nAs AI text sophistication increases, future research focuses on modeling the\n<i>dynamics of perplexity variance</i> and blending it with semantic and emotional irregularity\nfor reliable, explainable AI‚Äëvs‚Äëhuman classification.\n</p>\n\n\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset (adjust file name if needed)\ndf = pd.read_csv('/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv')\n\n# Quick structural glance\nprint(df.shape)          # (rows, columns)\nprint(df.columns)        # column names\ndf.head()                # display first 5 rows\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:58:11.984940Z","iopub.execute_input":"2025-11-13T19:58:11.985526Z","iopub.status.idle":"2025-11-13T19:58:13.498886Z","shell.execute_reply.started":"2025-11-13T19:58:11.985501Z","shell.execute_reply":"2025-11-13T19:58:13.498277Z"}},"outputs":[{"name":"stdout","text":"(44868, 5)\nIndex(['text', 'label', 'prompt_name', 'source', 'RDizzl3_seven'], dtype='object')\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                                                text  label  \\\n0  Phones\\n\\nModern humans today are always on th...      0   \n1  This essay will explain if drivers should or s...      0   \n2  Driving while the use of cellular devices\\n\\nT...      0   \n3  Phones & Driving\\n\\nDrivers should not be able...      0   \n4  Cell Phone Operation While Driving\\n\\nThe abil...      0   \n\n          prompt_name           source  RDizzl3_seven  \n0  Phones and driving  persuade_corpus          False  \n1  Phones and driving  persuade_corpus          False  \n2  Phones and driving  persuade_corpus          False  \n3  Phones and driving  persuade_corpus          False  \n4  Phones and driving  persuade_corpus          False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>prompt_name</th>\n      <th>source</th>\n      <th>RDizzl3_seven</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Phones\\n\\nModern humans today are always on th...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This essay will explain if drivers should or s...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Driving while the use of cellular devices\\n\\nT...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# you can get all unique prompt names:\nunique_prompts = df['prompt_name'].unique()\nprint(unique_prompts)\n\n# now split df into smaller DataFrames\nprompt_dfs = {}\n\nfor prompt in unique_prompts:\n    prompt_dfs[prompt] = df[df['prompt_name'] == prompt].copy()\n\n# Example access:\ndistance_learning_df = prompt_dfs['Distance learning']\ncar_free_df = prompt_dfs['Car-free cities']\nelectoral_df = prompt_dfs['Does the electoral college work?']\n\n# Quick verification\nfor name, subdf in prompt_dfs.items():\n    print(f\"{name}: {subdf.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:58:16.548937Z","iopub.execute_input":"2025-11-13T19:58:16.549284Z","iopub.status.idle":"2025-11-13T19:58:16.614594Z","shell.execute_reply.started":"2025-11-13T19:58:16.549261Z","shell.execute_reply":"2025-11-13T19:58:16.613890Z"}},"outputs":[{"name":"stdout","text":"['Phones and driving' 'Car-free cities' 'Summer projects'\n '\"A Cowboy Who Rode the Waves\"' 'Mandatory extracurricular activities'\n 'Exploring Venus' 'Facial action coding system' 'The Face on Mars'\n 'Community service' 'Grades for extracurricular activities'\n 'Driverless cars' 'Does the electoral college work?'\n 'Cell phones at school' 'Distance learning' 'Seeking multiple opinions']\nPhones and driving: (1583, 5)\nCar-free cities: (4717, 5)\nSummer projects: (2701, 5)\n\"A Cowboy Who Rode the Waves\": (1896, 5)\nMandatory extracurricular activities: (3077, 5)\nExploring Venus: (2176, 5)\nFacial action coding system: (3084, 5)\nThe Face on Mars: (1893, 5)\nCommunity service: (2092, 5)\nGrades for extracurricular activities: (2116, 5)\nDriverless cars: (2250, 5)\nDoes the electoral college work?: (4434, 5)\nCell phones at school: (2119, 5)\nDistance learning: (5554, 5)\nSeeking multiple opinions: (5176, 5)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"electoral_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\n\n# Setup \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\nmodel.eval()\n\ndef compute_perplexity(text):\n    \"\"\"Compute perplexity for a single text sample.\"\"\"\n    with torch.no_grad():\n        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n        input_ids = enc.input_ids.to(device)\n        attention_mask = enc.attention_mask.to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n        loss = outputs.loss\n        return torch.exp(loss).item()\n\n# Topic Selection \nselected_topics = [\n    \"Distance learning\",\n    \"Car-free cities\",\n    \"Does the electoral college work?\"\n]\n\nselected_dfs = {topic: df[df[\"prompt_name\"] == topic].copy() for topic in selected_topics}\n\n#  Perplexity by Class \nstats_per_topic = {}\n\nfor topic, topic_df in selected_dfs.items():\n    stats_per_topic[topic] = {}\n    for label_val, class_name in zip([1, 0], [\"AI\", \"Human\"]):\n        sub_df = topic_df[topic_df[\"label\"] == label_val]\n        pp_values = sub_df[\"text\"].apply(compute_perplexity)\n        stats_per_topic[topic][class_name] = {\n            \"mean\": pp_values.mean(),\n            \"std\": pp_values.std(),\n            \"count\": len(pp_values)\n        }\n\n# Summary Table\nresult_df = pd.DataFrame([\n    {\n        \"prompt_name\": topic,\n        \"AI_mean_PP\": stats[\"AI\"][\"mean\"],\n        \"AI_std_PP\": stats[\"AI\"][\"std\"],\n        \"AI_count\": stats[\"AI\"][\"count\"],\n        \"Human_mean_PP\": stats[\"Human\"][\"mean\"],\n        \"Human_std_PP\": stats[\"Human\"][\"std\"],\n        \"Human_count\": stats[\"Human\"][\"count\"],\n    }\n    for topic, stats in stats_per_topic.items()\n])\n\ndisplay(result_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T20:13:00.608443Z","iopub.execute_input":"2025-11-13T20:13:00.609541Z","iopub.status.idle":"2025-11-13T20:19:04.575460Z","shell.execute_reply.started":"2025-11-13T20:13:00.609511Z","shell.execute_reply":"2025-11-13T20:19:04.574550Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                        prompt_name  AI_mean_PP  AI_std_PP  AI_count  \\\n0                 Distance learning   10.195369   5.025817      3397   \n1                   Car-free cities    9.484958   6.152635      2051   \n2  Does the electoral college work?    9.267250   5.251502      1720   \n\n   Human_mean_PP  Human_std_PP  Human_count  \n0      28.757841     15.570113         2157  \n1      43.356668     22.414555         2666  \n2      33.620551     17.076914         2714  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt_name</th>\n      <th>AI_mean_PP</th>\n      <th>AI_std_PP</th>\n      <th>AI_count</th>\n      <th>Human_mean_PP</th>\n      <th>Human_std_PP</th>\n      <th>Human_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Distance learning</td>\n      <td>10.195369</td>\n      <td>5.025817</td>\n      <td>3397</td>\n      <td>28.757841</td>\n      <td>15.570113</td>\n      <td>2157</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Car-free cities</td>\n      <td>9.484958</td>\n      <td>6.152635</td>\n      <td>2051</td>\n      <td>43.356668</td>\n      <td>22.414555</td>\n      <td>2666</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Does the electoral college work?</td>\n      <td>9.267250</td>\n      <td>5.251502</td>\n      <td>1720</td>\n      <td>33.620551</td>\n      <td>17.076914</td>\n      <td>2714</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\n\nrecords = []\n\nfor topic, class_data in stats_per_topic.items():\n    for class_name, metrics in class_data.items():\n        # Handle both old and new key naming conventions safely\n        mean_pp = metrics.get(\"mean_perplexity\", metrics.get(\"mean\", 0))\n        std_pp  = metrics.get(\"std_perplexity\", metrics.get(\"std\", 0))\n        b_vmr   = metrics.get(\"burstiness_VMR\", metrics.get(\"burstiness_vmr\", 0))\n        b_cv    = metrics.get(\"burstiness_CV\", metrics.get(\"burstiness_cv\", 0))\n        count   = metrics.get(\"sample_count\", metrics.get(\"count\", 0))\n        \n        record = {\n            \"Topic\": topic,\n            \"Class\": class_name,\n            \"Mean‚ÄØPerplexity\": round(mean_pp, 4),\n            \"Std‚ÄØPerplexity\": round(std_pp, 4),\n            \"Burstiness‚ÄØVMR\": round(b_vmr, 4),\n            \"Burstiness‚ÄØCV\": round(b_cv, 4),\n            \"Samples\": count\n        }\n        records.append(record)\n\ndf_summary = pd.DataFrame(records)\ndf_summary.sort_values(by=[\"Topic\", \"Class\"], inplace=True)\n\n# Display neatly by topic\nfor topic, sub_df in df_summary.groupby(\"Topic\"):\n    print(f\"\\n‚îÄ‚îÄ‚îÄ {topic.upper()} ‚îÄ‚îÄ‚îÄ\")\n    display(sub_df.drop(columns=[\"Topic\"]).reset_index(drop=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T20:25:30.128526Z","iopub.execute_input":"2025-11-13T20:25:30.128800Z","iopub.status.idle":"2025-11-13T20:25:30.162706Z","shell.execute_reply.started":"2025-11-13T20:25:30.128780Z","shell.execute_reply":"2025-11-13T20:25:30.162109Z"}},"outputs":[{"name":"stdout","text":"\n‚îÄ‚îÄ‚îÄ CAR-FREE CITIES ‚îÄ‚îÄ‚îÄ\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Class  Mean‚ÄØPerplexity  Std‚ÄØPerplexity  Burstiness‚ÄØVMR  Burstiness‚ÄØCV  \\\n0     AI           9.4850          6.1526             0.0            0.0   \n1  Human          43.3567         22.4146             0.0            0.0   \n\n   Samples  \n0     2051  \n1     2666  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>Mean‚ÄØPerplexity</th>\n      <th>Std‚ÄØPerplexity</th>\n      <th>Burstiness‚ÄØVMR</th>\n      <th>Burstiness‚ÄØCV</th>\n      <th>Samples</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AI</td>\n      <td>9.4850</td>\n      <td>6.1526</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2051</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Human</td>\n      <td>43.3567</td>\n      <td>22.4146</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2666</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n‚îÄ‚îÄ‚îÄ DISTANCE LEARNING ‚îÄ‚îÄ‚îÄ\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Class  Mean‚ÄØPerplexity  Std‚ÄØPerplexity  Burstiness‚ÄØVMR  Burstiness‚ÄØCV  \\\n0     AI          10.1954          5.0258             0.0            0.0   \n1  Human          28.7578         15.5701             0.0            0.0   \n\n   Samples  \n0     3397  \n1     2157  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>Mean‚ÄØPerplexity</th>\n      <th>Std‚ÄØPerplexity</th>\n      <th>Burstiness‚ÄØVMR</th>\n      <th>Burstiness‚ÄØCV</th>\n      <th>Samples</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AI</td>\n      <td>10.1954</td>\n      <td>5.0258</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3397</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Human</td>\n      <td>28.7578</td>\n      <td>15.5701</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2157</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n‚îÄ‚îÄ‚îÄ DOES THE ELECTORAL COLLEGE WORK? ‚îÄ‚îÄ‚îÄ\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Class  Mean‚ÄØPerplexity  Std‚ÄØPerplexity  Burstiness‚ÄØVMR  Burstiness‚ÄØCV  \\\n0     AI           9.2673          5.2515          0.0000         0.0000   \n1  Human          33.6206         17.0769          8.6739         0.5079   \n\n   Samples  \n0     1720  \n1     2714  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>Mean‚ÄØPerplexity</th>\n      <th>Std‚ÄØPerplexity</th>\n      <th>Burstiness‚ÄØVMR</th>\n      <th>Burstiness‚ÄØCV</th>\n      <th>Samples</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AI</td>\n      <td>9.2673</td>\n      <td>5.2515</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>1720</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Human</td>\n      <td>33.6206</td>\n      <td>17.0769</td>\n      <td>8.6739</td>\n      <td>0.5079</td>\n      <td>2714</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"<div style=\"\n    background-color:#1c1c1c;\n    color:#f5f5f5;\n    padding:35px;\n    border-radius:10px;\n    font-family:'Segoe UI','Helvetica Neue',sans-serif;\n    line-height:1.8;\n    font-size:15px;\n\">\n\n<h1 style=\"text-align:center; color:#ffd54f;\">üß© Observation: Validity of GPT‚Äë2 for Perplexity‚ÄëBased Experiments</h1>\n\n<p>\nIn this experiment, we questioned whether using the <b>GPT‚Äë2</b> model for computing perplexity \nremains scientifically valid‚Äîgiven the release of newer models like GPT‚ÄëNeo, LLaMA, and Mistral. \nThe analysis confirms that GPT‚Äë2 is still a <b>valid and reliable reference model</b> for \nperplexity‚Äëbased AI vs Human detection tasks, mainly due to its stability and interpretability.\n</p>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 1. Why GPT‚Äë2 Remains Valid</h2>\n\n<ul>\n    <li><b>Consistency Over Recency:</b> Perplexity measures depend on having a fixed, \n        reproducible reference model‚Äînot on having the latest model. GPT‚Äë2‚Äôs \n        stable probability distribution ensures consistency across runs and datasets.</li>\n    <li><b>Transparent Probability Outputs:</b> Unlike newer instruction‚Äëtuned models, GPT‚Äë2 \n        exposes clean next‚Äëtoken likelihoods, making its PP scores more mathematically interpretable.</li>\n    <li><b>Benchmark Reliability:</b> Many academic and Kaggle implementations (2023‚Äì2025) \n        still use GPT‚Äë2 to extract PP features because it is \n        <b>computationally light, reproducible, and well‚Äëdocumented</b>.</li>\n</ul>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 2. When to Consider Newer Models</h2>\n\n<p>\nWhile GPT‚Äë2 is fully valid for comparative PP analysis, more advanced models can offer benefits\nin certain cases:\n</p>\n<table style=\"width:100%; border-collapse:collapse; margin-top:8px;\">\n<thead>\n<tr style=\"background-color:#2a2a2a; color:#ffd54f;\">\n  <th style=\"padding:8px; border-bottom:1px solid #444;\">Model</th>\n  <th style=\"padding:8px; border-bottom:1px solid #444;\">Improvement</th>\n  <th style=\"padding:8px; border-bottom:1px solid #444;\">Limitations</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">GPT‚ÄëNeo / GPT‚ÄëJ</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Larger vocab and smoother PP curves</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Slower scoring; high VRAM usage</td>\n</tr>\n<tr>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">LLaMA‚Äë2 / LLaMA‚Äë3</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Modern architecture, better long‚Äëcontext modeling</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Tokenizer mismatch; limited open PP support</td>\n</tr>\n<tr>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Mistral / Mixtral</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">More calibrated probabilities for newer datasets</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Too large for Kaggle runtime without quantization</td>\n</tr>\n</tbody>\n</table>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 3. Best Practice for Kaggle Experiments</h2>\n\n<ul>\n    <li>Start with <b>GPT‚Äë2 or GPT‚Äë2‚ÄëMedium</b> as your reference for sentence‚Äëlevel PP scoring.</li>\n    <li>Optionally validate results by comparing with a secondary model like GPT‚ÄëNeo \n        or OPT‚Äë1.3B to ensure consistent PP distribution profiles.</li>\n    <li>Ensure all PP statistics (mean, std, range, skew) are computed within the \n        same reference model for each dataset to maintain fair comparison.</li>\n</ul>\n\n<hr style=\"border:0.5px solid #333;\">\n\n<h2 style=\"color:#ffb74d;\">üîπ 4. Overall Conclusion</h2>\n\n<p>\n<b>GPT‚Äë2 remains a scientifically valid and practical choice</b> for perplexity‚Äëbased AI vs Human \ndetection. Its interpretability, reproducibility, and manageable size outweigh its age. \nUpgrading to newer models is beneficial only for specialized or domain‚Äëadapted experiments, \nnot for baseline PP feature extraction.\n</p>\n\n\n\n</div>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}