{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/bert-model-aigen-detection?scriptVersionId=280923230\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import re\nimport unicodedata\nimport pandas as pd \nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nfrom transformers import BertModel\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:36:56.987979Z","iopub.execute_input":"2025-11-22T11:36:56.988254Z","iopub.status.idle":"2025-11-22T11:37:27.067278Z","shell.execute_reply.started":"2025-11-22T11:36:56.988233Z","shell.execute_reply":"2025-11-22T11:37:27.066674Z"}},"outputs":[{"name":"stderr","text":"2025-11-22 11:37:10.115992: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763811430.303486      93 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763811430.356219      93 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:37:27.068546Z","iopub.execute_input":"2025-11-22T11:37:27.068986Z","iopub.status.idle":"2025-11-22T11:37:29.096169Z","shell.execute_reply.started":"2025-11-22T11:37:27.068967Z","shell.execute_reply":"2025-11-22T11:37:29.095399Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import re\nimport unicodedata\n\ndef preprocess_text(text):\n    # 1. Basic Safety Check\n    if not isinstance(text, str):\n        return \"\"\n    \n    # 2. Unicode Normalization\n    # This fixes weird characters (like \"smart quotes\" vs \"straight quotes\")\n    # to a standard format so the model doesn't get confused by encoding.\n    text = unicodedata.normalize('NFKC', text)\n    \n    # 3. Remove HTML tags (if any exist in the dataset)\n    # This removes things like <br>, <div>, <p>\n    text = re.sub(r'<.*?>', '', text)\n    \n    # 4. Handle Newlines and Escape Characters\n    # You mentioned removing \\n. We replace them with a SPACE.\n    # This regex finds newlines (\\n), tabs (\\t), and carriage returns (\\r)\n    text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n    \n    # 5. Remove specific artifact tags (optional customization)\n    # Sometimes datasets have artifacts like \"[[uuid]]\" or similar.\n    # If you noticed specific ugly tags, add them here. \n    # For now, we clean extra backslashes that might be escape artifacts.\n    text = text.replace('\\\\', '')\n\n    # 6. Collapse multiple spaces into one\n    # \"Hello    world\" -> \"Hello world\"\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n\ndf['clean_text'] = df['text'].apply(preprocess_text)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:37:29.097063Z","iopub.execute_input":"2025-11-22T11:37:29.097453Z","iopub.status.idle":"2025-11-22T11:37:36.429984Z","shell.execute_reply.started":"2025-11-22T11:37:29.097427Z","shell.execute_reply":"2025-11-22T11:37:36.429409Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:37:36.431528Z","iopub.execute_input":"2025-11-22T11:37:36.431853Z","iopub.status.idle":"2025-11-22T11:37:36.458337Z","shell.execute_reply.started":"2025-11-22T11:37:36.431833Z","shell.execute_reply":"2025-11-22T11:37:36.457546Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                text  label  \\\n0  Phones\\n\\nModern humans today are always on th...      0   \n1  This essay will explain if drivers should or s...      0   \n2  Driving while the use of cellular devices\\n\\nT...      0   \n3  Phones & Driving\\n\\nDrivers should not be able...      0   \n4  Cell Phone Operation While Driving\\n\\nThe abil...      0   \n\n          prompt_name           source  RDizzl3_seven  \\\n0  Phones and driving  persuade_corpus          False   \n1  Phones and driving  persuade_corpus          False   \n2  Phones and driving  persuade_corpus          False   \n3  Phones and driving  persuade_corpus          False   \n4  Phones and driving  persuade_corpus          False   \n\n                                          clean_text  \n0  Phones Modern humans today are always on their...  \n1  This essay will explain if drivers should or s...  \n2  Driving while the use of cellular devices Toda...  \n3  Phones & Driving Drivers should not be able to...  \n4  Cell Phone Operation While Driving The ability...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>prompt_name</th>\n      <th>source</th>\n      <th>RDizzl3_seven</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Phones\\n\\nModern humans today are always on th...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n      <td>Phones Modern humans today are always on their...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This essay will explain if drivers should or s...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n      <td>This essay will explain if drivers should or s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Driving while the use of cellular devices\\n\\nT...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n      <td>Driving while the use of cellular devices Toda...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n      <td>Phones &amp; Driving Drivers should not be able to...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n      <td>Cell Phone Operation While Driving The ability...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# In the tokenization step, first I use the wordpieace method, no good results ? then use BPE.","metadata":{}},{"cell_type":"code","source":"\n\n# --- CONFIGURATION ---\nMODEL_NAME = 'bert-base-cased'  # Using Cased to capture capitalization signals\nMAX_LEN = 512\nBATCH_SIZE = 16                 # Adjust based on your GPU RAM (8, 16, or 32).Good for kaggle env\n\n\nclass AI_Detection_Dataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,      # Adds [CLS] and [SEP]\n            max_length=self.max_len,      # Sets limit to 512\n            return_token_type_ids=False,\n            padding='max_length',         # Pads shorter sentences to 512\n            truncation=True,              # Truncates longer sentences to 512\n            return_attention_mask=True,\n            return_tensors='pt',          # Returns PyTorch tensors\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\ndef prepare_dataloaders(df):\n    print(f\"Loading Tokenizer: {MODEL_NAME}...\")\n    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n    \n    df_train, df_val = train_test_split(\n        df, \n        test_size=0.2, \n        random_state=42, \n        stratify=df['label']\n    )\n    \n    train_dataset = AI_Detection_Dataset(\n        texts=df_train.clean_text.to_numpy(),\n        labels=df_train.label.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n    )\n    \n    val_dataset = AI_Detection_Dataset(\n        texts=df_val.clean_text.to_numpy(),\n        labels=df_val.label.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n    )\n    \n    # --- THE FIX IS HERE ---\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=True,\n        num_workers=2,      # CHANGED FROM 0 TO 2 (Uses 2 CPU cores to prep data)\n        pin_memory=True     # NEW: Speeds up transfer from RAM to GPU\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=2,      # CHANGED FROM 0 TO 2\n        pin_memory=True     # NEW\n    )\n    \n    return train_loader, val_loader\n\n# Re-initialize the loaders\ntrain_dataloader, val_dataloader = prepare_dataloaders(df)\nprint(\"Loaders optimized and ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:37:36.459742Z","iopub.execute_input":"2025-11-22T11:37:36.460106Z","iopub.status.idle":"2025-11-22T11:37:38.037912Z","shell.execute_reply.started":"2025-11-22T11:37:36.460074Z","shell.execute_reply":"2025-11-22T11:37:38.037222Z"}},"outputs":[{"name":"stdout","text":"Loading Tokenizer: bert-base-cased...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b200187b9d7f44b0b0c6d159ee4612d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"039dea2bf4c8438eb664975f51ac2be5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bccc95e408a4eb4b5e1019c25115b45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1291bb8f81145e2916f6b6c4e5bc1a5"}},"metadata":{}},{"name":"stdout","text":"Loaders optimized and ready.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class BERTClassifier(nn.Module):\n    def __init__(self, model_name):\n        super(BERTClassifier, self).__init__()\n        \n        # 1. Load the pre-trained BERT model\n        # This downloads the weights from Hugging Face\n        self.bert = BertModel.from_pretrained(model_name)\n        \n        # 2. Define the \"Drop Out\" layer\n        # This randomly turns off 30% of neurons during training to prevent overfitting\n        self.drop = nn.Dropout(p=0.3)\n        \n        # 3. Define the Output Layer (The Classification Head)\n        # 768 is the standard output size of bert-base\n        # 2 is the number of classes (Human vs AI)\n        self.out = nn.Linear(in_features=768, out_features=2)\n        \n    def forward(self, input_ids, attention_mask):\n        \n        # output[0] = sequence_output (states for all tokens)\n        # output[1] = pooled_output (a summary vector of the whole sentence)\n        output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # We use pooled_output because we want a classification for the *whole* text\n        pooled_output = output[1]\n        \n        #  Apply Dropout\n        output = self.drop(pooled_output)\n        \n        #  Pass through the final layer to get scores for Human vs AI\n        return self.out(output)\n\n# --- INITIALIZATION ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nmodel = BERTClassifier(MODEL_NAME)\nmodel = model.to(device)  # Move the entire model onto the GPU\n\nprint(\"Model initialized and moved to GPU successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:37:38.038709Z","iopub.execute_input":"2025-11-22T11:37:38.03891Z","iopub.status.idle":"2025-11-22T11:37:40.719884Z","shell.execute_reply.started":"2025-11-22T11:37:38.038893Z","shell.execute_reply":"2025-11-22T11:37:40.719243Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df4fb3b934c6415b90ee160f8ac1ef0d"}},"metadata":{}},{"name":"stdout","text":"Model initialized and moved to GPU successfully.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nfrom tqdm import tqdm\n\n# --- HYPERPARAMETERS ---\nEPOCHS = 3\nLEARNING_RATE = 2e-5\n\n# 1. Define Optimizer & Scheduler\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\ntotal_steps = len(train_dataloader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\n\n# 2. Define Loss Function\nloss_fn = CrossEntropyLoss().to(device)\n\n# --- TRAIN FUNCTION (Updated to use d['labels']) ---\ndef train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    \n    for d in tqdm(data_loader):\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        # FIX: Your dataset uses 'labels', not 'targets'\n        targets = d[\"labels\"].to(device) \n        \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n    return correct_predictions.double() / n_examples, np.mean(losses)\n\n# --- EVAL FUNCTION (Updated to use d['labels']) ---\ndef eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            # FIX: Your dataset uses 'labels', not 'targets'\n            targets = d[\"labels\"].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            \n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            \n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n            \n    return correct_predictions.double() / n_examples, np.mean(losses)\n\n# --- EXECUTE TRAINING ---\nhistory = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n\nprint(\"Starting training...\")\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n    \n    train_acc, train_loss = train_epoch(\n        model,\n        train_dataloader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(train_dataloader.dataset) # Gets precise length\n    )\n    \n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    \n    val_acc, val_loss = eval_model(\n        model,\n        val_dataloader,\n        loss_fn,\n        device,\n        len(val_dataloader.dataset)\n    )\n    \n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    torch.save(model.state_dict(), 'bert_model_state.bin')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:37:40.72072Z","iopub.execute_input":"2025-11-22T11:37:40.720935Z","iopub.status.idle":"2025-11-22T13:37:40.278411Z","shell.execute_reply.started":"2025-11-22T11:37:40.720918Z","shell.execute_reply":"2025-11-22T13:37:40.27745Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nEpoch 1/3\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2244/2244 [37:39<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.04596849410558395 accuracy 0.9885217585111719\nVal   loss 0.018951839609095737 accuracy 0.9952083797637621\n\nEpoch 2/3\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2244/2244 [37:41<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.009152057859294873 accuracy 0.9980219535298379\nVal   loss 0.014982730877941752 accuracy 0.996434143080009\n\nEpoch 3/3\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2244/2244 [37:43<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.002085809821947447 accuracy 0.9995263832395387\nVal   loss 0.025695181519913506 accuracy 0.9953198127925118\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ndef get_predictions(model, data_loader):\n    model = model.eval()\n    predictions = []\n    prediction_probs = []\n    real_values = []\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"labels\"].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            \n            _, preds = torch.max(outputs, dim=1)\n            \n            predictions.extend(preds)\n            prediction_probs.extend(outputs)\n            real_values.extend(targets)\n            \n    predictions = torch.stack(predictions).cpu()\n    prediction_probs = torch.stack(prediction_probs).cpu()\n    real_values = torch.stack(real_values).cpu()\n    \n    return predictions, prediction_probs, real_values\n\n# 1. Get predictions on Validation Set\nprint(\"Generating predictions for evaluation...\")\ny_pred, y_pred_probs, y_test = get_predictions(\n    model,\n    val_dataloader\n)\n\n# 2. Print Classification Report\nprint(\"\\n--- Classification Report ---\")\n# Assuming 0=Human, 1=AI (or vice versa depending on your dataset setup)\nprint(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))\n\n# 3. Plot Confusion Matrix\ndef show_confusion_matrix(confusion_matrix):\n    plt.figure(figsize=(8, 6))\n    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.title('Confusion Matrix')\n    plt.show()\n\ncm = confusion_matrix(y_test, y_pred)\nshow_confusion_matrix(cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T13:53:40.741074Z","iopub.execute_input":"2025-11-22T13:53:40.741559Z"}},"outputs":[{"name":"stdout","text":"Generating predictions for evaluation...\n","output_type":"stream"}],"execution_count":null}]}