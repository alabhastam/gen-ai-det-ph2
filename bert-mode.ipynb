{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport unicodedata\nimport pandas as pd \nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nfrom transformers import BertModel\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:01:09.210847Z","iopub.execute_input":"2025-11-21T07:01:09.211608Z","iopub.status.idle":"2025-11-21T07:01:23.372035Z","shell.execute_reply.started":"2025-11-21T07:01:09.211556Z","shell.execute_reply":"2025-11-21T07:01:23.371171Z"}},"outputs":[{"name":"stderr","text":"2025-11-21 07:01:15.186504: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763708475.211067     107 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763708475.218328     107 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport unicodedata\n\ndef preprocess_text(text):\n    # 1. Basic Safety Check\n    if not isinstance(text, str):\n        return \"\"\n    \n    # 2. Unicode Normalization\n    # This fixes weird characters (like \"smart quotes\" vs \"straight quotes\")\n    # to a standard format so the model doesn't get confused by encoding.\n    text = unicodedata.normalize('NFKC', text)\n    \n    # 3. Remove HTML tags (if any exist in the dataset)\n    # This removes things like <br>, <div>, <p>\n    text = re.sub(r'<.*?>', '', text)\n    \n    # 4. Handle Newlines and Escape Characters\n    # You mentioned removing \\n. We replace them with a SPACE.\n    # This regex finds newlines (\\n), tabs (\\t), and carriage returns (\\r)\n    text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n    \n    # 5. Remove specific artifact tags (optional customization)\n    # Sometimes datasets have artifacts like \"[[uuid]]\" or similar.\n    # If you noticed specific ugly tags, add them here. \n    # For now, we clean extra backslashes that might be escape artifacts.\n    text = text.replace('\\\\', '')\n\n    # 6. Collapse multiple spaces into one\n    # \"Hello    world\" -> \"Hello world\"\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n\ndf['clean_text'] = df['text'].apply(preprocess_text)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# In the tokenization step, first I use the wordpieace method, no good results ? then use BPE.","metadata":{}},{"cell_type":"code","source":"\n\n# --- CONFIGURATION ---\nMODEL_NAME = 'bert-base-cased'  # Using Cased to capture capitalization signals\nMAX_LEN = 512\nBATCH_SIZE = 16                 # Adjust based on your GPU RAM (8, 16, or 32).Good for kaggle env\n\n\nclass AI_Detection_Dataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,      # Adds [CLS] and [SEP]\n            max_length=self.max_len,      # Sets limit to 512\n            return_token_type_ids=False,\n            padding='max_length',         # Pads shorter sentences to 512\n            truncation=True,              # Truncates longer sentences to 512\n            return_attention_mask=True,\n            return_tensors='pt',          # Returns PyTorch tensors\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\ndef prepare_dataloaders(df):\n    print(f\"Loading Tokenizer: {MODEL_NAME}...\")\n    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n    \n    # Split into Train (80%) and Validation (20%)\n    # Using 'stratify' ensures we keep the same Human/AI ratio in both sets\n    df_train, df_val = train_test_split(\n        df, \n        test_size=0.2, \n        random_state=42, \n        stratify=df['label']\n    )\n    \n    print(f\"Training Samples: {len(df_train)}\")\n    print(f\"Validation Samples: {len(df_val)}\")\n    \n    # Create Dataset Objects\n    # We use the 'clean_text' column you prepared, and 'label'\n    train_dataset = AI_Detection_Dataset(\n        texts=df_train.clean_text.to_numpy(),\n        labels=df_train.label.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n    )\n    \n    val_dataset = AI_Detection_Dataset(\n        texts=df_val.clean_text.to_numpy(), # fair enough . We used clean text.\n        labels=df_val.label.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n    )\n    \n    # Create DataLoaders\n    # These manage the batches during training\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=True,      # Shuffle training data every epoch\n        num_workers=0      # Set to 2 or 4 if on Linux for speed\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=BATCH_SIZE,\n        shuffle=False      # No need to shuffle validation\n    )\n    \n    return train_loader, val_loader\n\n# --- USAGE ---\n# Assuming 'cleaned_df' is the dataframe from the previous step\n# train_loader, val_loader = prepare_dataloaders(cleaned_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BERTClassifier(nn.Module):\n    def __init__(self, model_name):\n        super(BERTClassifier, self).__init__()\n        \n        # 1. Load the pre-trained BERT model\n        # This downloads the weights from Hugging Face\n        self.bert = BertModel.from_pretrained(model_name)\n        \n        # 2. Define the \"Drop Out\" layer\n        # This randomly turns off 30% of neurons during training to prevent overfitting\n        self.drop = nn.Dropout(p=0.3)\n        \n        # 3. Define the Output Layer (The Classification Head)\n        # 768 is the standard output size of bert-base\n        # 2 is the number of classes (Human vs AI)\n        self.out = nn.Linear(in_features=768, out_features=2)\n        \n    def forward(self, input_ids, attention_mask):\n        # A. Pass data through BERT\n        # output[0] = sequence_output (states for all tokens)\n        # output[1] = pooled_output (a summary vector of the whole sentence)\n        output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # We use pooled_output because we want a classification for the *whole* text\n        pooled_output = output[1]\n        \n        # B. Apply Dropout\n        output = self.drop(pooled_output)\n        \n        # C. Pass through the final layer to get scores for Human vs AI\n        return self.out(output)\n\n# --- INITIALIZATION ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nmodel = BERTClassifier(MODEL_NAME)\nmodel = model.to(device)  # Move the entire model onto the GPU\n\nprint(\"Model initialized and moved to GPU successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}