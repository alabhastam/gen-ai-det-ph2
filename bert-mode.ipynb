{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/bert-model-aigen-detection?scriptVersionId=280918694\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import re\nimport unicodedata\nimport pandas as pd \nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nfrom transformers import BertModel\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:20:38.788643Z","iopub.execute_input":"2025-11-22T11:20:38.788993Z","iopub.status.idle":"2025-11-22T11:20:38.793841Z","shell.execute_reply.started":"2025-11-22T11:20:38.788969Z","shell.execute_reply":"2025-11-22T11:20:38.793085Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:20:38.796065Z","iopub.execute_input":"2025-11-22T11:20:38.796538Z","iopub.status.idle":"2025-11-22T11:20:40.002143Z","shell.execute_reply.started":"2025-11-22T11:20:38.796518Z","shell.execute_reply":"2025-11-22T11:20:40.00154Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import re\nimport unicodedata\n\ndef preprocess_text(text):\n    # 1. Basic Safety Check\n    if not isinstance(text, str):\n        return \"\"\n    \n    # 2. Unicode Normalization\n    # This fixes weird characters (like \"smart quotes\" vs \"straight quotes\")\n    # to a standard format so the model doesn't get confused by encoding.\n    text = unicodedata.normalize('NFKC', text)\n    \n    # 3. Remove HTML tags (if any exist in the dataset)\n    # This removes things like <br>, <div>, <p>\n    text = re.sub(r'<.*?>', '', text)\n    \n    # 4. Handle Newlines and Escape Characters\n    # You mentioned removing \\n. We replace them with a SPACE.\n    # This regex finds newlines (\\n), tabs (\\t), and carriage returns (\\r)\n    text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n    \n    # 5. Remove specific artifact tags (optional customization)\n    # Sometimes datasets have artifacts like \"[[uuid]]\" or similar.\n    # If you noticed specific ugly tags, add them here. \n    # For now, we clean extra backslashes that might be escape artifacts.\n    text = text.replace('\\\\', '')\n\n    # 6. Collapse multiple spaces into one\n    # \"Hello    world\" -> \"Hello world\"\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n\ndf['clean_text'] = df['text'].apply(preprocess_text)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:20:40.003154Z","iopub.execute_input":"2025-11-22T11:20:40.003418Z","iopub.status.idle":"2025-11-22T11:20:48.228905Z","shell.execute_reply.started":"2025-11-22T11:20:40.003389Z","shell.execute_reply":"2025-11-22T11:20:48.228186Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:20:48.229703Z","iopub.execute_input":"2025-11-22T11:20:48.229943Z","iopub.status.idle":"2025-11-22T11:20:48.240009Z","shell.execute_reply.started":"2025-11-22T11:20:48.229924Z","shell.execute_reply":"2025-11-22T11:20:48.239214Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                                text  label  \\\n0  Phones\\n\\nModern humans today are always on th...      0   \n1  This essay will explain if drivers should or s...      0   \n2  Driving while the use of cellular devices\\n\\nT...      0   \n3  Phones & Driving\\n\\nDrivers should not be able...      0   \n4  Cell Phone Operation While Driving\\n\\nThe abil...      0   \n\n          prompt_name           source  RDizzl3_seven  \\\n0  Phones and driving  persuade_corpus          False   \n1  Phones and driving  persuade_corpus          False   \n2  Phones and driving  persuade_corpus          False   \n3  Phones and driving  persuade_corpus          False   \n4  Phones and driving  persuade_corpus          False   \n\n                                          clean_text  \n0  Phones Modern humans today are always on their...  \n1  This essay will explain if drivers should or s...  \n2  Driving while the use of cellular devices Toda...  \n3  Phones & Driving Drivers should not be able to...  \n4  Cell Phone Operation While Driving The ability...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>prompt_name</th>\n      <th>source</th>\n      <th>RDizzl3_seven</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Phones\\n\\nModern humans today are always on th...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n      <td>Phones Modern humans today are always on their...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This essay will explain if drivers should or s...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n      <td>This essay will explain if drivers should or s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Driving while the use of cellular devices\\n\\nT...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n      <td>Driving while the use of cellular devices Toda...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n      <td>Phones &amp; Driving Drivers should not be able to...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n      <td>0</td>\n      <td>Phones and driving</td>\n      <td>persuade_corpus</td>\n      <td>False</td>\n      <td>Cell Phone Operation While Driving The ability...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# In the tokenization step, first I use the wordpieace method, no good results ? then use BPE.","metadata":{}},{"cell_type":"code","source":"\n\n# --- CONFIGURATION ---\nMODEL_NAME = 'bert-base-cased'  # Using Cased to capture capitalization signals\nMAX_LEN = 512\nBATCH_SIZE = 16                 # Adjust based on your GPU RAM (8, 16, or 32).Good for kaggle env\n\n\nclass AI_Detection_Dataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,      # Adds [CLS] and [SEP]\n            max_length=self.max_len,      # Sets limit to 512\n            return_token_type_ids=False,\n            padding='max_length',         # Pads shorter sentences to 512\n            truncation=True,              # Truncates longer sentences to 512\n            return_attention_mask=True,\n            return_tensors='pt',          # Returns PyTorch tensors\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\ndef prepare_dataloaders(df):\n    print(f\"Loading Tokenizer: {MODEL_NAME}...\")\n    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n    \n    df_train, df_val = train_test_split(\n        df, \n        test_size=0.2, \n        random_state=42, \n        stratify=df['label']\n    )\n    \n    train_dataset = AI_Detection_Dataset(\n        texts=df_train.clean_text.to_numpy(),\n        labels=df_train.label.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n    )\n    \n    val_dataset = AI_Detection_Dataset(\n        texts=df_val.clean_text.to_numpy(),\n        labels=df_val.label.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n    )\n    \n    # --- THE FIX IS HERE ---\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=True,\n        num_workers=2,      # CHANGED FROM 0 TO 2 (Uses 2 CPU cores to prep data)\n        pin_memory=True     # NEW: Speeds up transfer from RAM to GPU\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=2,      # CHANGED FROM 0 TO 2\n        pin_memory=True     # NEW\n    )\n    \n    return train_loader, val_loader\n\n# Re-initialize the loaders\ntrain_dataloader, val_dataloader = prepare_dataloaders(df)\nprint(\"Loaders optimized and ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:28:55.672234Z","iopub.execute_input":"2025-11-22T11:28:55.672783Z","iopub.status.idle":"2025-11-22T11:28:56.064849Z","shell.execute_reply.started":"2025-11-22T11:28:55.672743Z","shell.execute_reply":"2025-11-22T11:28:56.063916Z"}},"outputs":[{"name":"stdout","text":"Loading Tokenizer: bert-base-cased...\nLoaders optimized and ready.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"class BERTClassifier(nn.Module):\n    def __init__(self, model_name):\n        super(BERTClassifier, self).__init__()\n        \n        # 1. Load the pre-trained BERT model\n        # This downloads the weights from Hugging Face\n        self.bert = BertModel.from_pretrained(model_name)\n        \n        # 2. Define the \"Drop Out\" layer\n        # This randomly turns off 30% of neurons during training to prevent overfitting\n        self.drop = nn.Dropout(p=0.3)\n        \n        # 3. Define the Output Layer (The Classification Head)\n        # 768 is the standard output size of bert-base\n        # 2 is the number of classes (Human vs AI)\n        self.out = nn.Linear(in_features=768, out_features=2)\n        \n    def forward(self, input_ids, attention_mask):\n        \n        # output[0] = sequence_output (states for all tokens)\n        # output[1] = pooled_output (a summary vector of the whole sentence)\n        output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # We use pooled_output because we want a classification for the *whole* text\n        pooled_output = output[1]\n        \n        #  Apply Dropout\n        output = self.drop(pooled_output)\n        \n        #  Pass through the final layer to get scores for Human vs AI\n        return self.out(output)\n\n# --- INITIALIZATION ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nmodel = BERTClassifier(MODEL_NAME)\nmodel = model.to(device)  # Move the entire model onto the GPU\n\nprint(\"Model initialized and moved to GPU successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:29:00.408053Z","iopub.execute_input":"2025-11-22T11:29:00.408363Z","iopub.status.idle":"2025-11-22T11:29:00.791206Z","shell.execute_reply.started":"2025-11-22T11:29:00.40834Z","shell.execute_reply":"2025-11-22T11:29:00.790317Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nModel initialized and moved to GPU successfully.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nfrom tqdm import tqdm\n\n# --- HYPERPARAMETERS ---\nEPOCHS = 3\nLEARNING_RATE = 2e-5\n\n# 1. Define Optimizer & Scheduler\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\ntotal_steps = len(train_dataloader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\n\n# 2. Define Loss Function\nloss_fn = CrossEntropyLoss().to(device)\n\n# --- TRAIN FUNCTION (Updated to use d['labels']) ---\ndef train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    \n    for d in tqdm(data_loader):\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        # FIX: Your dataset uses 'labels', not 'targets'\n        targets = d[\"labels\"].to(device) \n        \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n    return correct_predictions.double() / n_examples, np.mean(losses)\n\n# --- EVAL FUNCTION (Updated to use d['labels']) ---\ndef eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            # FIX: Your dataset uses 'labels', not 'targets'\n            targets = d[\"labels\"].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            \n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            \n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n            \n    return correct_predictions.double() / n_examples, np.mean(losses)\n\n# --- EXECUTE TRAINING ---\nhistory = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n\nprint(\"Starting training...\")\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n    \n    train_acc, train_loss = train_epoch(\n        model,\n        train_dataloader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(train_dataloader.dataset) # Gets precise length\n    )\n    \n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    \n    val_acc, val_loss = eval_model(\n        model,\n        val_dataloader,\n        loss_fn,\n        device,\n        len(val_dataloader.dataset)\n    )\n    \n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    torch.save(model.state_dict(), 'bert_model_state.bin')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:29:12.059085Z","iopub.execute_input":"2025-11-22T11:29:12.059365Z","iopub.status.idle":"2025-11-22T11:29:56.701733Z","shell.execute_reply.started":"2025-11-22T11:29:12.059344Z","shell.execute_reply":"2025-11-22T11:29:56.700442Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nEpoch 1/3\n----------\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 24/2244 [00:44<1:08:42,  1.86s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_96/1841172593.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     train_acc, train_loss = train_epoch(\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_96/1841172593.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# FIX: Your dataset uses 'labels', not 'targets'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"history = {\n    'train_acc': [],\n    'train_loss': [],\n    'val_acc': [],\n    'val_loss': []\n}\n\nprint(\"Starting training...\")\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n    \n    # 1. Train\n    train_acc, train_loss = train_epoch(\n        model,\n        train_dataloader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(df_train)\n    )\n    \n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    \n    # 2. Validate\n    val_acc, val_loss = eval_model(\n        model,\n        val_dataloader,\n        loss_fn,\n        device,\n        len(df_val)\n    )\n    \n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    \n    # Store history\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    # Save the model state just in case\n    torch.save(model.state_dict(), 'bert_model_state.bin')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:20:48.8181Z","iopub.status.idle":"2025-11-22T11:20:48.818317Z","shell.execute_reply.started":"2025-11-22T11:20:48.818211Z","shell.execute_reply":"2025-11-22T11:20:48.81822Z"}},"outputs":[],"execution_count":null}]}