{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"\n    background-color:#1e1e1e;\n    color:#f5f5f5;\n    padding:35px;\n    border-radius:12px;\n    font-family:'Segoe UI', 'Helvetica Neue', sans-serif;\n    line-height:1.7;\n    font-size:15px;\n\">\n\n<h1 style=\"text-align:center; color:#ffc107;\">ğŸ§© Understanding Perplexity in Language Models</h1>\n\n<p>\n<b>Perplexity (PP)</b> quantifies how confidently a language model predicts the next token.  \nItâ€™s essentially the modelâ€™s <i>â€œdegree of surprise.â€</i>  \nA low perplexity means smooth, natural predictions â€” a high one means confusion or uncertainty.\n</p>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">ğŸ“˜ Mathematical Definition</h2>\n\n<p style=\"text-align:center;\">\nPP = exp(H) = exp( âˆ’ (1/N) Î£ log P(w<sub>i</sub> | w<sub>1</sub>, ..., w<sub>iâˆ’1</sub>) )\n</p>\n\n<ul>\n  <li><b>P(w<sub>i</sub> | w<sub>1</sub>, ..., w<sub>iâˆ’1</sub>)</b>: Probability of the next token.</li>\n  <li><b>N</b>: Number of words in the sequence.</li>\n  <li><b>logâ€¯P</b>: Log-likelihood, later exponentiated to get back to normal scale.</li>\n</ul>\n\n<p><i>Lower PP â†’ more predictable and fluent text.<br>Higher PP â†’ randomness or topic drift.</i></p>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">ğŸ“Š Numerical Interpretation</h2>\n\n<table style=\"width:100%; border-collapse:collapse; margin-top:8px;\">\n<thead>\n<tr style=\"background-color:#2a2a2a; color:#ffd54f;\">\n  <th style=\"padding:8px; border-bottom:1px solid #444;\">Perplexity Range</th>\n  <th style=\"padding:8px; border-bottom:1px solid #444;\">Meaning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">10 â€“ 25</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Excellent â€” coherent and humanâ€‘like fluency.</td>\n</tr>\n<tr>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">30 â€“ 70</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Moderate â€” the model struggles slightly with context.</td>\n</tr>\n<tr>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">&gt; 100</td>\n  <td style=\"padding:8px; border-bottom:1px solid #333;\">Poor â€” the model is confused or offâ€‘domain.</td>\n</tr>\n</tbody>\n</table>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">âš™ï¸ Use Cases in NLP</h2>\n\n<ol>\n<li><b>Model Evaluation:</b> Compare language models â€” the lower the PP, the better.</li>\n<li><b>AI vs Human Detection:</b> AI text = low & stable PP. Human text = high & variable PP.</li>\n<li><b>Dataset Filtering:</b> Identify unnatural or corrupted samples during preprocessing.</li>\n</ol>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">ğŸ§  Intuitive Example</h2>\n\n<p>If a model predicts the next word in <code style=\"background-color:#2f2f2f; color:#fff;\">I love ___</code>:</p>\n\n<ul>\n<li><b>â€œyouâ€</b> with probability 0.9 â†’ low PP (expected, fluent).</li>\n<li><b>â€œbananasâ€</b> with probability 0.01 â†’ high PP (unexpected, offâ€‘context).</li>\n</ul>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">ğŸ Example in Python (Kaggleâ€‘Ready)</h2>\n\n<pre style=\"background-color:#2b2b2b; color:#e0e0e0; padding:14px; border-radius:8px; overflow-x:auto;\">\n<code class=\"language-python\">\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nimport torch, math\n\n# Load GPT-2\nmodel_name = \"gpt2\"\ntokenizer = GPT2TokenizerFast.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Example text\ntext = \"I love you so much.\"\n\n# Encode & evaluate\ninputs = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\nperplexity = math.exp(loss)\n\nprint(f\"Perplexity: {perplexity:.2f}\")\n</code>\n</pre>\n\n<hr style=\"border:0.5px solid #444;\">\n\n<h2 style=\"color:#ffb74d;\">ğŸ§© Summary</h2>\n\n<ul>\n<li><b>Definition:</b> Statistical indicator of prediction confidence.</li>\n<li><b>Goal:</b> Lower perplexity = stronger model quality.</li>\n<li><b>AI Detection Insight:</b> Human writing â†’ variable PP; AI â†’ smooth & uniform PP.</li>\n</ul>\n\n\n\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}