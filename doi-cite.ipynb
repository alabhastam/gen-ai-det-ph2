{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9097,"sourceType":"datasetVersion","datasetId":491}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Why Checking DOIs via API is the â€œSilver Bulletâ€ for AI Detection\nChecking References, specifically through their Digital Object Identifiers (DOIs), is arguably the most definitive method to catch AI hallucinations. Large Language Models (LLMs) like ChatGPT often generate plausible-sounding citations that do not actually exist.\n\nHere is why the Python + doi.org Content Negotiation method is superior:\n\n-  Deterministic Accuracy (Binary Result)\nUnlike analyzing writing style or â€œperplexityâ€ scoresâ€”which are probabilistic and prone to false positivesâ€”a DOI check is binary. A DOI either exists in the global registry, or it doesnâ€™t.\n\nResult: 404 Not Found = 100% Fake Reference.\n\n - Detecting â€œStolenâ€ DOIs\nAI sometimes hallucinates by taking a real DOI from an unrelated paper and attaching it to a fake citation.\n\n- The Fix: By retrieving the metadata (JSON) directly from the source, you can compare the actual title in the database against the title listed in the suspicious paper. If the paper claims to be about â€œEconomicsâ€ but the DOI resolves to â€œMarine Biology,â€ it is undeniable proof of AI generation.\n-  Global Coverage (Not Just One Publisher)\nBy querying the central doi.org resolver rather than specific publisher APIs (like Elsevier or Wiley), this method covers all academic content.\n\nEfficiency: It handles redirects automatically, finding the metadata whether the paper is hosted on Crossref, DataCite, or mEDRA.\n- . Scalability and Automation\nManually clicking 50 links is tedious. This Python script allows for batch processing. You can feed it a list of 100 references and receive a full audit report in seconds, making it perfect for editors, professors, or automated quality control systems.","metadata":{}},{"cell_type":"markdown","source":"In this section, we proved that this is an efficient way to find if a paper is valid or not. ","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport time","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\n\ndef verify_doi_validity(doi_input):\n    \"\"\"\n    Checks if a DOI exists by querying the doi.org resolver directly.\n    Returns detailed metadata if valid, or an error status if invalid.\n    \"\"\"\n    # Clean the input to ensure we only have the DOI string\n    clean_doi = doi_input.replace(\"https://doi.org/\", \"\").replace(\"http://doi.org/\", \"\")\n    \n    url = f\"https://doi.org/{clean_doi}\"\n    \n    headers = {\n        \"Accept\": \"application/vnd.citationstyles.csl+json\"\n    }\n\n    try:\n        response = requests.get(url, headers=headers, allow_redirects=True, timeout=10)\n        \n        if response.status_code == 200:\n            try:\n                data = response.json()\n            except ValueError:\n                return {\"status\": \"Error\", \"details\": \"Response was not valid JSON.\"}\n            \n            # 1. Extracting Title\n            title = data.get('title', 'N/A')\n            if isinstance(title, list) and len(title) > 0:\n                title = title[0]\n            \n            # 2. Extracting Journal Name (Container Title)\n            journal = data.get('container-title', 'N/A')\n            if isinstance(journal, list) and len(journal) > 0:\n                journal = journal[0]\n\n            # 3. Extracting First Author's Last Name\n            author_lastname = \"N/A\"\n            if 'author' in data and len(data['author']) > 0:\n                # We take the first author in the list\n                author_lastname = data['author'][0].get('family', 'N/A')\n\n            return {\n                \"status\": \"Valid\",\n                \"real_title\": title,\n                \"journal\": journal,\n                \"first_author\": author_lastname\n            }\n            \n        elif response.status_code == 404:\n            return {\"status\": \"Invalid\", \"details\": \"DOI not found\"}\n        else:\n            return {\"status\": \"Error\", \"details\": f\"HTTP Code: {response.status_code}\"}\n\n    except Exception as e:\n        return {\"status\": \"Connection Error\", \"details\": str(e)}\n\n# --- Usage Example ---\n\ndoi_list_to_check = [\n    \"10.1038/nature123\",            # Fake\n    \"10.1007/s10701-005-9016-x\",    # Valid (Physics paper)\n    \"10.1016/j.jbi.2008.04.002\",    # Valid (Bioinformatics paper)\n    \"10.1126/science.fake.999\"      # Fake\n]\n\n# Header format for the table\nprint(f\"{'DOI':<27} | {'Status':<8} | {'Author':<15} | {'Journal':<20} | {'Real Title'}\")\nprint(\"-\" * 110)\n\nfor doi in doi_list_to_check:\n    result = verify_doi_validity(doi)\n    \n    if result['status'] == \"Valid\":\n        # Clean and shorten strings for table display\n        author = str(result['first_author'])[:15]\n        journal = str(result['journal'])[:20]\n        title = str(result['real_title'])[:35] + \"...\"\n        \n        print(f\"{doi:<27} | {result['status']:<8} | {author:<15} | {journal:<20} | {title}\")\n    else:\n        # For errors, we just print the details in the last column\n        print(f\"{doi:<27} | {result['status']:<8} | {'-':<15} | {'-':<20} | {result.get('details', '-')}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# for csv files","metadata":{}},{"cell_type":"code","source":"#  Define the extraction function\ndef extract_dois_from_text(text):\n    \"\"\"\n    Scans a text string for DOIs using regex.\n    Returns a list of unique DOIs found, or an empty list.\n    \"\"\"\n    # The standard DOI regex\n    doi_pattern = r'\\b(10\\.\\d{4,9}/[-._;()/:a-zA-Z0-9]+)\\b'\n    # we can extend \\d{4,9} mybe capture more \n    \n    if not isinstance(text, str):\n        return []\n        \n    matches = re.findall(doi_pattern, text)\n    \n    # Clean up trailing punctuation (like a period at the end of a sentence)\n    unique_dois = set()\n    for doi in matches:\n        clean = doi.rstrip(\".,)\")\n        unique_dois.add(clean)\n        \n    return list(unique_dois)\n\n#  Apply it to the dataframe\nprint(\"Extracting DOIs from 'paper_text' column... this might take a moment.\")\ndf['extracted_dois'] = df['paper_text'].apply(extract_dois_from_text)\n\n#  Create a count column just to see how many we found per paper\ndf['doi_count'] = df['extracted_dois'].apply(len)\n\n# 4. Filter to show only papers where we actually found DOIs\npapers_with_dois = df[df['doi_count'] > 0].copy()\n\nprint(f\"\\nProcessing Complete.\")\nprint(f\"Total Papers Scanned: {len(df)}\")\nprint(f\"Papers containing DOIs: {len(papers_with_dois)}\")\n\n# Show a preview of the results\nif len(papers_with_dois) > 0:\n    print(\"\\n--- Preview of Papers with Extracted DOIs ---\")\n    # We select just the ID, Year, Title, and the list of DOIs found\n    display_cols = ['id', 'year', 'title', 'extracted_dois']\n    try:\n        display(papers_with_dois[display_cols].head())\n    except NameError:\n        print(papers_with_dois[display_cols].head())\nelse:\n    print(\"No DOIs found. Note: Older papers (1987-1990s) often didn't print DOIs in their bibliographies.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Your provided verification function\ndef verify_doi_validity(doi_input):\n    clean_doi = doi_input.replace(\"https://doi.org/\", \"\").replace(\"http://doi.org/\", \"\")\n    url = f\"https://doi.org/{clean_doi}\"\n    headers = {\"Accept\": \"application/vnd.citationstyles.csl+json\"}\n\n    try:\n        response = requests.get(url, headers=headers, allow_redirects=True, timeout=10)\n        \n        if response.status_code == 200:\n            try:\n                data = response.json()\n            except ValueError:\n                return {\"status\": \"Error\", \"details\": \"Response was not valid JSON.\"}\n            \n            title = data.get('title', 'N/A')\n            if isinstance(title, list) and len(title) > 0: title = title[0]\n            \n            journal = data.get('container-title', 'N/A')\n            if isinstance(journal, list) and len(journal) > 0: journal = journal[0]\n\n            author_lastname = \"N/A\"\n            if 'author' in data and len(data['author']) > 0:\n                author_lastname = data['author'][0].get('family', 'N/A')\n\n            return {\n                \"validity\": \"Valid\",\n                \"meta_title\": title,\n                \"meta_journal\": journal,\n                \"meta_author\": author_lastname,\n                \"details\": \"OK\"\n            }\n        elif response.status_code == 404:\n            return {\"validity\": \"Invalid\", \"meta_title\": \"-\", \"meta_journal\": \"-\", \"meta_author\": \"-\", \"details\": \"DOI Not Found\"}\n        else:\n            return {\"validity\": \"Error\", \"meta_title\": \"-\", \"meta_journal\": \"-\", \"meta_author\": \"-\", \"details\": f\"HTTP {response.status_code}\"}\n\n    except Exception as e:\n        return {\"validity\": \"Conn Error\", \"meta_title\": \"-\", \"meta_journal\": \"-\", \"meta_author\": \"-\", \"details\": str(e)}\n\n\n# Iterate through the papers and check their DOIs\n\nresults_list = []\n\n# LIMITER: We only check the first 5 papers for this demo to save time.\n# Remove .head(5) to run on all papers.\npapers_to_check = papers_with_dois.head(5)\n\nprint(f\"Starting verification on {len(papers_to_check)} papers...\")\n\nfor index, row in papers_to_check.iterrows():\n    paper_id = row['id']\n    paper_year = row['year']\n    extracted_dois = row['extracted_dois']\n    \n    print(f\"Processing Paper ID {paper_id} ({len(extracted_dois)} DOIs found)...\")\n    \n    for doi in extracted_dois:\n        # Run the verification API\n        res = verify_doi_validity(doi)\n        \n        # Save the result in a structured way\n        results_list.append({\n            \"Paper_ID\": paper_id,\n            \"Paper_Year\": paper_year,\n            \"Checked_DOI\": doi,\n            \"Status\": res['validity'],\n            \"Real_Author\": res['meta_author'],\n            \"Real_Journal\": res['meta_journal'],\n            \"Real_Title\": res['meta_title'],\n            \"Notes\": res['details']\n        })\n        \n        # Be polite to the API server, sleep a tiny bit\n        time.sleep(0.2)\n\n# Convert results to a DataFrame for nice display\nverification_df = pd.DataFrame(results_list)\n\nprint(\"\\n--- Verification Complete ---\")\n\n# Display valid vs invalid counts\nprint(verification_df['Status'].value_counts())\n\nprint(\"\\n--- Detailed Results Table ---\")\n# Displaying in a nice clean format\ndisplay_cols = ['Paper_ID', 'Checked_DOI', 'Status', 'Real_Author', 'Real_Journal']\ntry:\n    display(verification_df[display_cols])\nexcept NameError:\n    print(verification_df[display_cols])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gradio Mini App","metadata":{}},{"cell_type":"code","source":"import gradio as gr\nimport re\n\ndef extract_dois(text):\n    \"\"\"\n    Parses the input text and uses regex to find all unique DOIs.\n    \"\"\"\n    if not text:\n        return \"Please paste some text to analyze.\"\n\n    # Regex Explanation:\n    # \\b(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)\n    # \\b        : Word boundary (ensures we don't match the middle of a string randomly)\n    # 10\\.      : All DOIs start with \"10.\"\n    # \\d{4,9}   : The registrant code (usually 4 or 5 digits, but can be longer)\n    # /         : Separation between prefix and suffix\n    # [...]     : Allowed characters in the suffix (letters, numbers, dashes, dots, etc.)\n    # re.IGNORECASE : DOIs are case-insensitive\n    \n    # This pattern catches both raw \"10.xxx/yyy\" and URLs containing it\n    doi_pattern = r'\\b(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)\\b'\n    \n    matches = re.findall(doi_pattern, text, flags=re.IGNORECASE)\n    \n    # Deduplicate results (using set) and sort them\n    unique_dois = sorted(list(set(matches)))\n    \n    if not unique_dois:\n        return \"No DOIs found in the provided text.\"\n    \n    # Format the output as a numbered list\n    result_text = f\"Found {len(unique_dois)} unique DOI(s):\\n\\n\"\n    for i, doi in enumerate(unique_dois, 1):\n        # Clean trailing punctuation that regex might accidentally grab (like a period at end of sentence)\n        clean_doi = doi.rstrip('.')\n        result_text += f\"{i}. {clean_doi}\\n\"\n        \n    return result_text\n\n# --- Gradio Interface Setup ---\n\nwith gr.Blocks(title=\"DOI Extractor\") as demo:\n    gr.Markdown(\"# ðŸ“„ Research Paper DOI Extractor\")\n    gr.Markdown(\"Paste the full text of a paper or bibliography below to extract all Mentioned DOIs via Regex.\")\n    \n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(\n                lines=15, \n                placeholder=\"Paste paper text here...\", \n                label=\"Input Text\"\n            )\n            extract_btn = gr.Button(\"Find DOIs\", variant=\"primary\")\n            \n        with gr.Column():\n            output_text = gr.Textbox(\n                lines=15, \n                label=\"Extracted DOIs\", \n                interactive=False\n            )\n    \n    # Event Listener\n    extract_btn.click(fn=extract_dois, inputs=input_text, outputs=output_text)\n\nif __name__ == \"__main__\":\n    demo.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:30:42.802858Z","iopub.execute_input":"2025-11-30T16:30:42.803854Z","iopub.status.idle":"2025-11-30T16:30:50.004094Z","shell.execute_reply.started":"2025-11-30T16:30:42.803823Z","shell.execute_reply":"2025-11-30T16:30:50.003168Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://292c24e0a1d10793c5.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://292c24e0a1d10793c5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":1}]}